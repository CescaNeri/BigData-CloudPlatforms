const local_index = {"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Course and Exam Details Matteo Francia = Manager of the course, Cloud Platforms Enrico Gallinucci = Big Data (specialized in social data) Program Module 1 : Theoretical background on Big Data Module 2 : Handling big data in the cloud (guided practice) Technologies used AWS Academy (Lab Exercises) Apache Spark Apache Hadoop Apache Kafka Python :heart: EXAM - Oral Examination Questions on all (theoretical and practical) aspects of the course, including both modules and seminars.","title":"Course and Exam Details"},{"location":"index.html#course-and-exam-details","text":"Matteo Francia = Manager of the course, Cloud Platforms Enrico Gallinucci = Big Data (specialized in social data)","title":"Course and Exam Details"},{"location":"index.html#program","text":"Module 1 : Theoretical background on Big Data Module 2 : Handling big data in the cloud (guided practice)","title":"Program"},{"location":"index.html#technologies-used","text":"AWS Academy (Lab Exercises) Apache Spark Apache Hadoop Apache Kafka Python :heart:","title":"Technologies used"},{"location":"index.html#exam-oral-examination","text":"Questions on all (theoretical and practical) aspects of the course, including both modules and seminars.","title":"EXAM - Oral Examination"},{"location":"SUMMARY.html","text":"Big Data Introduction Infrastructure and Architecture Distributed File Systems Batch Application","title":"SUMMARY"},{"location":"big-data/batch-application/SUMMARY.html","text":"YARN Map Reduce","title":"SUMMARY"},{"location":"big-data/batch-application/map-reduce.html","text":"Map Reduce It is based, in general, on key-value pairs (data is structured in couples). It is a programming model and an associated implementation for processing and generating large data sets. How it works: It is based on typical analytical problems, in which you have large datasets and you want to extract something, you reorganize the data to compute aggregations and generate a final output. Map operations and reduce operations MAP takes a function f and applies it to every element in a list FOLD iteratively applies a function g to aggregate results Parallelization The map operation includes all operations can be parallelized in a straightforward manner, since each functional application happens in isolation. Reduce operation has more restrictions on data locality. MapReduce Program The map function requires an input (key-value pairs), that are chosen by the programmer. Map (k1, v1) -> list(k2, v2) Reduce (k2, list(v2)) -> list(k3, v3) The output of the map function is a list containing all the values in the row, while the reduce function takes as input a key and the list with all the values associated with that key. MapReduce program = job Each job is divided into smaller units called tasks The tasks are scheduled using YARN and run on nodes in the cluster MapReduce Process Input is divided into fixed-size splits A Map task is created for each split The key-value pairs returned by Map tasks are sorted and stored in the local disk Map outputs are sent to the nodes where the Reduce tasks are running The key-value pairs returned by Reduce tasks are written persistently onto the DFS Example - Word Count Counting the number of occurrences for each word in a collection of documents. Input: a repository of documents (each document is a value in the input pairs) Map function: read a document and emit a sequence of key-value pairs Shuffle and Sort: group by key and generate a pairs of the form Reduce function: add up all the values for a given key and emits a pair of the form (w, m) Output: w is a word that appears at least once among all the input documents; m is the total number of occurrences of w among all those documents Map ( String docid , String text ): for each word w in text : Emit ( w , 1 ); Reduce ( String term , counts [] ): int sum = 0 ; for each c in counts : sum += c ; Emit ( term , sum ); Map in Java public class WordCountMapperextendsMapper < LongWritable , Text , Text , IntWritable > { private static final IntWritableone = newIntWritable ( 1 ); privateText word = newText (); public void map ( LongWritablekey , Text value , Context context ) throwsIOException , InterruptedException { String line = value . toString (); StringTokenizertokenizer = newStringTokenizer ( line ); while ( tokenizer . hasMoreTokens ()) { word . set ( tokenizer . nextToken ()); context . write ( word , one ); } } } Reduce in Java public class WordCountReducerextendsReducer < Text , IntWritable , Text , IntWritable > { public void reduce ( Text key , Iterable < IntWritable > values , Context context ) throwsIOException , InterruptedException { intsum = 0 ; for ( IntWritablevalue : values ) { sum += value . get (); } context . write ( key , newIntWritable ( sum )); } } Combiners When the reduce function is associative and commutative, we can push some of what the reducers do to the Map tasks. In this case, we also apply a combiner to the Map function. The combiner function must be associative and commutative. Advantages: It reduces the amount of intermediate data It reduces the network traffic Data Partitioning Maps: Partitioning depends on the input splits One map task per input split Reducers: Data is shuffled and according to partitioning function, it decides, for each key, which reducers it goes to Based on the number of nodes and available resources Can be defined by the user The keyspace of the intermediate key-value pairs is evenly distributed over the reducers with a hash function (same keys in different mappers end up at the same reducer). The partitioning of the intermediate key-value pairs generated by the maps can be customized. How does it work: Let p be the number of reduce tasks The partitioner adopts a hash function that translates a key to a number from 0 to p-1 Each output ket-value pair is stored in one of p files The reduce task collects from every map task the partitions with the same hash value Tuning the number of reduce tasks Hadoop used to create only one, global reduce task by default (mainly used). Deciding the number of reduce task is not easy; it is more an art than a science. One solution would be to devise single task for every CPU available in the cluster. In this way, all CPUs are working but in reality, there are often more keys to process than available CPUs. Another solution would be to create multiple tasks for each CPU. From a certain perspective, it may seem a waste but actually, this solution mitigate skewness problems. General rule of thumb: each task should run for about 5 minutes and produce more than 1 HDFS block's worth of output. MapReduce Execution An important idea behind MapReduce is separating the what of distributed processing from the how. The developer launches the job on the client's JMV, which contacts the YARN RM to submit the application. Data locality enforcement Main principle: do not move data to workers, move workers to data. The task are created from the input splits in the shared file system Optimization: prefer nodes that are on the same rack in the data center as the on holding the data block. Inter-rack bandwidth is significantly less than intra-rack bandwidth. Map Execution STEPS","title":"Map Reduce"},{"location":"big-data/batch-application/map-reduce.html#map-reduce","text":"It is based, in general, on key-value pairs (data is structured in couples). It is a programming model and an associated implementation for processing and generating large data sets. How it works: It is based on typical analytical problems, in which you have large datasets and you want to extract something, you reorganize the data to compute aggregations and generate a final output. Map operations and reduce operations MAP takes a function f and applies it to every element in a list FOLD iteratively applies a function g to aggregate results","title":"Map Reduce"},{"location":"big-data/batch-application/map-reduce.html#parallelization","text":"The map operation includes all operations can be parallelized in a straightforward manner, since each functional application happens in isolation. Reduce operation has more restrictions on data locality.","title":"Parallelization"},{"location":"big-data/batch-application/map-reduce.html#mapreduce-program","text":"The map function requires an input (key-value pairs), that are chosen by the programmer. Map (k1, v1) -> list(k2, v2) Reduce (k2, list(v2)) -> list(k3, v3) The output of the map function is a list containing all the values in the row, while the reduce function takes as input a key and the list with all the values associated with that key. MapReduce program = job Each job is divided into smaller units called tasks The tasks are scheduled using YARN and run on nodes in the cluster","title":"MapReduce Program"},{"location":"big-data/batch-application/map-reduce.html#mapreduce-process","text":"Input is divided into fixed-size splits A Map task is created for each split The key-value pairs returned by Map tasks are sorted and stored in the local disk Map outputs are sent to the nodes where the Reduce tasks are running The key-value pairs returned by Reduce tasks are written persistently onto the DFS","title":"MapReduce Process"},{"location":"big-data/batch-application/map-reduce.html#example-word-count","text":"Counting the number of occurrences for each word in a collection of documents. Input: a repository of documents (each document is a value in the input pairs) Map function: read a document and emit a sequence of key-value pairs Shuffle and Sort: group by key and generate a pairs of the form Reduce function: add up all the values for a given key and emits a pair of the form (w, m) Output: w is a word that appears at least once among all the input documents; m is the total number of occurrences of w among all those documents Map ( String docid , String text ): for each word w in text : Emit ( w , 1 ); Reduce ( String term , counts [] ): int sum = 0 ; for each c in counts : sum += c ; Emit ( term , sum );","title":"Example - Word Count"},{"location":"big-data/batch-application/map-reduce.html#map-in-java","text":"public class WordCountMapperextendsMapper < LongWritable , Text , Text , IntWritable > { private static final IntWritableone = newIntWritable ( 1 ); privateText word = newText (); public void map ( LongWritablekey , Text value , Context context ) throwsIOException , InterruptedException { String line = value . toString (); StringTokenizertokenizer = newStringTokenizer ( line ); while ( tokenizer . hasMoreTokens ()) { word . set ( tokenizer . nextToken ()); context . write ( word , one ); } } }","title":"Map in Java"},{"location":"big-data/batch-application/map-reduce.html#reduce-in-java","text":"public class WordCountReducerextendsReducer < Text , IntWritable , Text , IntWritable > { public void reduce ( Text key , Iterable < IntWritable > values , Context context ) throwsIOException , InterruptedException { intsum = 0 ; for ( IntWritablevalue : values ) { sum += value . get (); } context . write ( key , newIntWritable ( sum )); } }","title":"Reduce in Java"},{"location":"big-data/batch-application/map-reduce.html#combiners","text":"When the reduce function is associative and commutative, we can push some of what the reducers do to the Map tasks. In this case, we also apply a combiner to the Map function. The combiner function must be associative and commutative. Advantages: It reduces the amount of intermediate data It reduces the network traffic","title":"Combiners"},{"location":"big-data/batch-application/map-reduce.html#data-partitioning","text":"Maps: Partitioning depends on the input splits One map task per input split Reducers: Data is shuffled and according to partitioning function, it decides, for each key, which reducers it goes to Based on the number of nodes and available resources Can be defined by the user The keyspace of the intermediate key-value pairs is evenly distributed over the reducers with a hash function (same keys in different mappers end up at the same reducer). The partitioning of the intermediate key-value pairs generated by the maps can be customized. How does it work: Let p be the number of reduce tasks The partitioner adopts a hash function that translates a key to a number from 0 to p-1 Each output ket-value pair is stored in one of p files The reduce task collects from every map task the partitions with the same hash value Tuning the number of reduce tasks Hadoop used to create only one, global reduce task by default (mainly used). Deciding the number of reduce task is not easy; it is more an art than a science. One solution would be to devise single task for every CPU available in the cluster. In this way, all CPUs are working but in reality, there are often more keys to process than available CPUs. Another solution would be to create multiple tasks for each CPU. From a certain perspective, it may seem a waste but actually, this solution mitigate skewness problems. General rule of thumb: each task should run for about 5 minutes and produce more than 1 HDFS block's worth of output.","title":"Data Partitioning"},{"location":"big-data/batch-application/map-reduce.html#mapreduce-execution","text":"An important idea behind MapReduce is separating the what of distributed processing from the how. The developer launches the job on the client's JMV, which contacts the YARN RM to submit the application. Data locality enforcement Main principle: do not move data to workers, move workers to data. The task are created from the input splits in the shared file system Optimization: prefer nodes that are on the same rack in the data center as the on holding the data block. Inter-rack bandwidth is significantly less than intra-rack bandwidth.","title":"MapReduce Execution"},{"location":"big-data/batch-application/map-reduce.html#map-execution-steps","text":"","title":"Map Execution STEPS"},{"location":"big-data/batch-application/yarn.html","text":"YARN - Yet ANother Resource Negotiator In the same cluster, I want to run some programs and applications. The problem regards how these computations allocate resources. The resource negotiator: In charge of assigning units of work Has a global view of the resources available in the cluster Decides where and when each uint of work should be allocated and with how many resources Uses a scheduling policy to manage concurrency Avoids over-instantiation of processes Handles fault-tolerance Potentially, we can run as many process as we want on each machine but we do not want to overload it. YARN provides APIs for requesting and working with cluster resources. It is a general framework (works with any kind of application you want to run) Application are written using analytical frameworks Main Daemons YARN work with the master/slave mechanism: Resource Manager (ultimate authority that arbitrates among all the applications) Node Manager (per-node slave) In charge of running containers which run applications (virtual abstract entities associated with a certain amount of resources. We have a virtual environment that run applications) For each application. there is one container that runs a special process that in charge of coordinating the single operation. Coordination happens on two levels: Resource Manager, composed by two components: Application Manager (start the application - accept request from client and start the machine) Scheduling component (decides where resources should be allocated) For each application, we have another process ( application master process ) that manages the resources of each application Application execution consists of the following steps: A client program submits the application, including the necessary specifications to launch the application-specific AMP itself The RM assumes the responsibility to negotiate a specified container in which to start the AMP and then launches it The AMP registers with the RM The AMP negotiates appropriate resources containers On successful container allocations, the AMP launches the container by providing the container launch specification The application code executing within the container provides necessary information During the application execution, the client that submitted the program communicates directly with the AMP to get status and updates Once the application is complete, the AMP de-registers with the RM and shuts down, allowing its own container to be repurposed YARN Scheduler YARN provides a choice of schedulers and configurable policies: FIFO scheduler (useless) Fair Scheduler (with multiple applications that want to access the resource, the scheduler will balance the resources among the two applications) Problem related to prediction of execution time as resources are split Capacity Scheduler (the amount of resources is divided in two profiles - fixed amount) Each application run within a profile have access to a fixed amount of resources Execution time is predictable but there may be a waste of resources Data Locality When the scheduler needs to identify the resources to allocate, it does so following the data locality principle. The point is to exploit cluster typology and data block replication to apply the data locality principle. When computation involves large set of data, it is cheaper to move code to data rather than data to code","title":"YARN"},{"location":"big-data/batch-application/yarn.html#yarn-yet-another-resource-negotiator","text":"In the same cluster, I want to run some programs and applications. The problem regards how these computations allocate resources. The resource negotiator: In charge of assigning units of work Has a global view of the resources available in the cluster Decides where and when each uint of work should be allocated and with how many resources Uses a scheduling policy to manage concurrency Avoids over-instantiation of processes Handles fault-tolerance Potentially, we can run as many process as we want on each machine but we do not want to overload it. YARN provides APIs for requesting and working with cluster resources. It is a general framework (works with any kind of application you want to run) Application are written using analytical frameworks","title":"YARN - Yet ANother Resource Negotiator"},{"location":"big-data/batch-application/yarn.html#main-daemons","text":"YARN work with the master/slave mechanism: Resource Manager (ultimate authority that arbitrates among all the applications) Node Manager (per-node slave) In charge of running containers which run applications (virtual abstract entities associated with a certain amount of resources. We have a virtual environment that run applications) For each application. there is one container that runs a special process that in charge of coordinating the single operation. Coordination happens on two levels: Resource Manager, composed by two components: Application Manager (start the application - accept request from client and start the machine) Scheduling component (decides where resources should be allocated) For each application, we have another process ( application master process ) that manages the resources of each application Application execution consists of the following steps: A client program submits the application, including the necessary specifications to launch the application-specific AMP itself The RM assumes the responsibility to negotiate a specified container in which to start the AMP and then launches it The AMP registers with the RM The AMP negotiates appropriate resources containers On successful container allocations, the AMP launches the container by providing the container launch specification The application code executing within the container provides necessary information During the application execution, the client that submitted the program communicates directly with the AMP to get status and updates Once the application is complete, the AMP de-registers with the RM and shuts down, allowing its own container to be repurposed","title":"Main Daemons"},{"location":"big-data/batch-application/yarn.html#yarn-scheduler","text":"YARN provides a choice of schedulers and configurable policies: FIFO scheduler (useless) Fair Scheduler (with multiple applications that want to access the resource, the scheduler will balance the resources among the two applications) Problem related to prediction of execution time as resources are split Capacity Scheduler (the amount of resources is divided in two profiles - fixed amount) Each application run within a profile have access to a fixed amount of resources Execution time is predictable but there may be a waste of resources","title":"YARN Scheduler"},{"location":"big-data/batch-application/yarn.html#data-locality","text":"When the scheduler needs to identify the resources to allocate, it does so following the data locality principle. The point is to exploit cluster typology and data block replication to apply the data locality principle. When computation involves large set of data, it is cheaper to move code to data rather than data to code","title":"Data Locality"},{"location":"big-data/distributed-file-system/SUMMARY.html","text":"Hadoop DFS File Formats","title":"SUMMARY"},{"location":"big-data/distributed-file-system/file-formats.html","text":"File Formats The hadoop ecosystem supports several different formats: Standard file formats Hadoop / BigData-specific formats Hadoop-specific formats involves several advantages: Serialization Splittability (if the file is split into multiple blocks, metadata headers allow to skip unnecessary I/O) Metadata is associated with each single block (the client can access individual blocks) Compression (if you want to compress the data, you use a compression mechanism that works on a block-level or at a record-level) There are many file formats - row-oriented : Sequence files Apache thrift (Facebook), Protocol buffers (Google) They both create a notion of schema that has to be used in order to read the data Some code specifies how the data are structured Apache Avro Each file contains its own schema definition (no need to share separately the schema of the file because it is included in the file itself) Column-oriented file formats This formats store data based on columns. Column-oriented formats are better suited for analytical scenarios. This because in a typical analytical query, you want to take data from a subset of columns and perform analysis. This format is not ideal for operational purposes (day-to-day operations in a database). There are several advantaged to column-oriented file formats: Better compression (similar values because data is more homogeneous) Reduced I/O for analytical queries Operate on encoded data There are many file formats: ORC Files Apache Parquet (general purpose) Parquet Besides storing data in a column format, it allows to store nested structures (like JSON) in a flat format. Data Model Nested attributes that have multiple values. Types: Group Primitive Frequency: Required Optional Repeated Unnesting Can we store nested data structures in a columnar format? we need to map the schema to a list of columns in a way that we can write records to flat columns and read them back to their original nested data structure. Each value is associated with two integers (repetition level and definition level) These integers allow to fully reconstruct the nested structures while still being able to store each primitive separately When I put values in a column formats, I do not know which value belongs to which record. Rebuilding the structure of the rows just by looking at the values cannot be done. The parquet format allows to reconstruct the message. Parquet uses a definition level that defines the level of definition (0,1,2,3...), dealing with optionality. This allows you to understand which values exist. Inside the hierarchy, some fields are mandatory and others are optional. Repetition Level The presence of repeated fields requires to store when new lists are starting in a column of values. Repetition level indicates at which level we have to create a new list for the current value. By using these two values (repetition and definition), I am able to fully reconstruct the message. Repetition and definition level cause overhead (they occupy space). The number of bits that has to be used is quite small (1,2,3... the number of levels) and in some case they can be omitted The repetition levels are stored in a column so they can be compressed Parquet file format We have a notion of row group -> the data are not fully columnar. We do not store all the values of each column continuously. Usually, we split the file into row groups (horizontal partitioning) and each block of rows store the data in a columnar way. COlumn chunk (chunk of the data for a particular column) All the values of each columns is further subdivided into pages. The size of the row group is set to the same size of the block (each block corresponds to a row). Data page size -> it can be tuned (smaller or larger) depending on what query we need to issue.","title":"File Formats"},{"location":"big-data/distributed-file-system/file-formats.html#file-formats","text":"The hadoop ecosystem supports several different formats: Standard file formats Hadoop / BigData-specific formats Hadoop-specific formats involves several advantages: Serialization Splittability (if the file is split into multiple blocks, metadata headers allow to skip unnecessary I/O) Metadata is associated with each single block (the client can access individual blocks) Compression (if you want to compress the data, you use a compression mechanism that works on a block-level or at a record-level) There are many file formats - row-oriented : Sequence files Apache thrift (Facebook), Protocol buffers (Google) They both create a notion of schema that has to be used in order to read the data Some code specifies how the data are structured Apache Avro Each file contains its own schema definition (no need to share separately the schema of the file because it is included in the file itself)","title":"File Formats"},{"location":"big-data/distributed-file-system/file-formats.html#column-oriented-file-formats","text":"This formats store data based on columns. Column-oriented formats are better suited for analytical scenarios. This because in a typical analytical query, you want to take data from a subset of columns and perform analysis. This format is not ideal for operational purposes (day-to-day operations in a database). There are several advantaged to column-oriented file formats: Better compression (similar values because data is more homogeneous) Reduced I/O for analytical queries Operate on encoded data There are many file formats: ORC Files Apache Parquet (general purpose)","title":"Column-oriented file formats"},{"location":"big-data/distributed-file-system/file-formats.html#parquet","text":"Besides storing data in a column format, it allows to store nested structures (like JSON) in a flat format. Data Model Nested attributes that have multiple values. Types: Group Primitive Frequency: Required Optional Repeated Unnesting Can we store nested data structures in a columnar format? we need to map the schema to a list of columns in a way that we can write records to flat columns and read them back to their original nested data structure. Each value is associated with two integers (repetition level and definition level) These integers allow to fully reconstruct the nested structures while still being able to store each primitive separately When I put values in a column formats, I do not know which value belongs to which record. Rebuilding the structure of the rows just by looking at the values cannot be done. The parquet format allows to reconstruct the message. Parquet uses a definition level that defines the level of definition (0,1,2,3...), dealing with optionality. This allows you to understand which values exist. Inside the hierarchy, some fields are mandatory and others are optional. Repetition Level The presence of repeated fields requires to store when new lists are starting in a column of values. Repetition level indicates at which level we have to create a new list for the current value. By using these two values (repetition and definition), I am able to fully reconstruct the message. Repetition and definition level cause overhead (they occupy space). The number of bits that has to be used is quite small (1,2,3... the number of levels) and in some case they can be omitted The repetition levels are stored in a column so they can be compressed Parquet file format We have a notion of row group -> the data are not fully columnar. We do not store all the values of each column continuously. Usually, we split the file into row groups (horizontal partitioning) and each block of rows store the data in a columnar way. COlumn chunk (chunk of the data for a particular column) All the values of each columns is further subdivided into pages. The size of the row group is set to the same size of the block (each block corresponds to a row). Data page size -> it can be tuned (smaller or larger) depending on what query we need to issue.","title":"Parquet"},{"location":"big-data/distributed-file-system/hadoop-dfs.html","text":"Hadoop Distributed File System The first thing that software has to provide is abstraction , as we want to interact with the disks of the machines as if it was a single machine. We want a single reference and we want to talk to a single entity. Master-Slave architecture A single entity works as a master and handles the storage of the data with a set of slaves that are running of single machines. The software needs to deal with big data : Files may be bigger than single disks We need to split files into smaller blocks and store them on different machines ALso, fault tolerance is of key importance: disks can fail, machines can be unreachable, but data should always be available. we can store multiple copies of each block HDFS - definition HDFS is filesystem designed for storing very large files with a streaming data access patterns, running on clusters of commodity hardware. Application that run on HDFS need streaming access to their data sets. I do not want to continuously update data with batch operations. The emphasis is on high throughput of data access rather than low latency of data access Blocks -> splitting files into block that range between 64MB and 1GB. We need blocks because files can be larger than disks. Why are blocks this big? We focus on giving a high throughput as large files split into many small blocks require a huge number of seeks. Master - Slave Abstraction Master service -> namenode Persistently maintains the filesystem tree Coordinates the storage on different machines Keeps in memory the location of each block for a given file ( block pool ) Slaves -> datanodes Store and retrieve blocks SPoF The namenode is a single point of failure: without it, the file system cannot be used. Backup solution : not the best solution Secondary NN Solution : separate machine in which a secondary namenode is installed and does some synchronization with the primary one (while being in a sleeping state). High Availability Most of the big data tools and frameworks use this term to reference solutions that answer to the problem of always having data available no matter what. HA indicates a system that can tolerate faults. Supported by configuring two separate machines as NNs: One is active (up and running) The other is in standby In the event of failure of the active NN, the standby NN takes over. It s a more complex solution in terms of resources and communication with the network, but it guarantees that in case of failure, there will not be any downtime. HDFS - federation There is no need to have a single file system, but we can have many systems on the same cluster. We can create multiple NN that manage different file systems. This has several advantages: Performance (if we have a large filesystem with lots of data, the NN can be a bottleneck in terms of answering to multiple requests of users. By dividing file systems with different NN that address different requests, performance will increase) Availability Scalability (when the file system is large, the metadata can become quite huge. The metadata block needs to be lowered in memory so splitting metadata among different machines allows scalability) Maintainability, Security and Flexibility HDFS - replication Whenever we want to save a file in the file system, this will be splitted in blocks and stored separately. Each data block is independently replicated at multiple DNs in order to improve performance and robustness. Nodes are organized in racks, that are organized in data centers. Hadoop models such concepts in a tree-liked fashion and computes the distance between nodes as their distance on the tree. The typical rule is to store the first replica on the node (n1) where the client issued the write command. Replica 2 is stores on a node (n2) in a rack (r2) different form n1. Replica 3 is stored on a node different from n2 but tha belongs to r2. Hadoop 3 alternative to simple replication: Each block is split across each data node It reduces data redundancy It requires less storage (less money) Faster writes (when we need to write data, the data to write are less) DISADVANTAGES: Higher CPU cost (whenever we want to access data, having split the blocks in further blocks, we need to rebuild the blocks) Loss of data locality (whenever we have a unit of work that is running on a certain machine, the system tries to instatiate this application as close to the data as possible. This works if all the data are in the same machine, but in this solution data are slit in different machines. Therefore, we cannot apply the data locality principle.) Longer recovery time HDFS not always the best fit Although this may change in the future, there are area where HDFS is not a good fit: Low-latency data access Lots of small files I/O communication An application client wishing to read a file must first contact the NN to determine where the actual data is stored. The NN identifies the relevant block The client contacts the DN to retrieve the data Features of design: The namenode never removes data All data transfer occurs directly between clients and DNs Communications with the NN only involve transfer of metadata","title":"Hadoop DFS"},{"location":"big-data/distributed-file-system/hadoop-dfs.html#hadoop-distributed-file-system","text":"The first thing that software has to provide is abstraction , as we want to interact with the disks of the machines as if it was a single machine. We want a single reference and we want to talk to a single entity. Master-Slave architecture A single entity works as a master and handles the storage of the data with a set of slaves that are running of single machines. The software needs to deal with big data : Files may be bigger than single disks We need to split files into smaller blocks and store them on different machines ALso, fault tolerance is of key importance: disks can fail, machines can be unreachable, but data should always be available. we can store multiple copies of each block","title":"Hadoop Distributed File System"},{"location":"big-data/distributed-file-system/hadoop-dfs.html#hdfs-definition","text":"HDFS is filesystem designed for storing very large files with a streaming data access patterns, running on clusters of commodity hardware. Application that run on HDFS need streaming access to their data sets. I do not want to continuously update data with batch operations. The emphasis is on high throughput of data access rather than low latency of data access Blocks -> splitting files into block that range between 64MB and 1GB. We need blocks because files can be larger than disks. Why are blocks this big? We focus on giving a high throughput as large files split into many small blocks require a huge number of seeks.","title":"HDFS - definition"},{"location":"big-data/distributed-file-system/hadoop-dfs.html#master-slave-abstraction","text":"Master service -> namenode Persistently maintains the filesystem tree Coordinates the storage on different machines Keeps in memory the location of each block for a given file ( block pool ) Slaves -> datanodes Store and retrieve blocks","title":"Master - Slave Abstraction"},{"location":"big-data/distributed-file-system/hadoop-dfs.html#spof","text":"The namenode is a single point of failure: without it, the file system cannot be used. Backup solution : not the best solution Secondary NN Solution : separate machine in which a secondary namenode is installed and does some synchronization with the primary one (while being in a sleeping state).","title":"SPoF"},{"location":"big-data/distributed-file-system/hadoop-dfs.html#high-availability","text":"Most of the big data tools and frameworks use this term to reference solutions that answer to the problem of always having data available no matter what. HA indicates a system that can tolerate faults. Supported by configuring two separate machines as NNs: One is active (up and running) The other is in standby In the event of failure of the active NN, the standby NN takes over. It s a more complex solution in terms of resources and communication with the network, but it guarantees that in case of failure, there will not be any downtime.","title":"High Availability"},{"location":"big-data/distributed-file-system/hadoop-dfs.html#hdfs-federation","text":"There is no need to have a single file system, but we can have many systems on the same cluster. We can create multiple NN that manage different file systems. This has several advantages: Performance (if we have a large filesystem with lots of data, the NN can be a bottleneck in terms of answering to multiple requests of users. By dividing file systems with different NN that address different requests, performance will increase) Availability Scalability (when the file system is large, the metadata can become quite huge. The metadata block needs to be lowered in memory so splitting metadata among different machines allows scalability) Maintainability, Security and Flexibility","title":"HDFS - federation"},{"location":"big-data/distributed-file-system/hadoop-dfs.html#hdfs-replication","text":"Whenever we want to save a file in the file system, this will be splitted in blocks and stored separately. Each data block is independently replicated at multiple DNs in order to improve performance and robustness. Nodes are organized in racks, that are organized in data centers. Hadoop models such concepts in a tree-liked fashion and computes the distance between nodes as their distance on the tree. The typical rule is to store the first replica on the node (n1) where the client issued the write command. Replica 2 is stores on a node (n2) in a rack (r2) different form n1. Replica 3 is stored on a node different from n2 but tha belongs to r2. Hadoop 3 alternative to simple replication: Each block is split across each data node It reduces data redundancy It requires less storage (less money) Faster writes (when we need to write data, the data to write are less) DISADVANTAGES: Higher CPU cost (whenever we want to access data, having split the blocks in further blocks, we need to rebuild the blocks) Loss of data locality (whenever we have a unit of work that is running on a certain machine, the system tries to instatiate this application as close to the data as possible. This works if all the data are in the same machine, but in this solution data are slit in different machines. Therefore, we cannot apply the data locality principle.) Longer recovery time","title":"HDFS - replication"},{"location":"big-data/distributed-file-system/hadoop-dfs.html#hdfs-not-always-the-best-fit","text":"Although this may change in the future, there are area where HDFS is not a good fit: Low-latency data access Lots of small files I/O communication An application client wishing to read a file must first contact the NN to determine where the actual data is stored. The NN identifies the relevant block The client contacts the DN to retrieve the data Features of design: The namenode never removes data All data transfer occurs directly between clients and DNs Communications with the NN only involve transfer of metadata","title":"HDFS not always the best fit"},{"location":"big-data/infrastructures-architectures/SUMMARY.html","text":"Big Data Infrastructure Big Data Architecture","title":"SUMMARY"},{"location":"big-data/infrastructures-architectures/architectures.html","text":"Big Data Architectures There are a lot of problems related to distributed architectures, especially concerning parallelization: Communication between workers Access to shared resources Parallelization and concurrency The solution is to have single computer -> Data Center The core is the framework provider , which is the set of software modules that handles the abstraction of the complexity of the computer system and makes it visible as if it was a single entity. System Orchestrator -> manages application (both a human being or a program) ADD DETAILS FROM THE SLIDES!!! 'til 28 ADD DETAILS Analytical Applications Batch Analysis: Take a large amount of data and run analysis Run on demand Takes a lot of time Stream Analysis We devised an algorithm that runs continuously Real-time results Useful for monitoring Can we run both batch and stream on the same set of data? Lambda architecture Most trivial way to handle this problem. It consists in duplicating the data in order to apply both analysis. Data goes through two path: Hot path (timely, real-time, less accurate data) Cold path (less timely but more accurate data) Kappa architecture We can simplify the problem and handle everything a stream problem. Stream and batch application are different BUT we can run batch analysis on a stream engine. Data flows through a single patch, using a stream processing system. Lambda vs Kappa Lambda is easier but it needs parallel development and maintenance of two parallel pipelines. Kappa is the ongoing trend. A streaming engine can handle a bounded dataset and a well-designed streaming systems provide a strict superset of batch functionality.","title":"Big Data Architecture"},{"location":"big-data/infrastructures-architectures/architectures.html#big-data-architectures","text":"There are a lot of problems related to distributed architectures, especially concerning parallelization: Communication between workers Access to shared resources Parallelization and concurrency The solution is to have single computer -> Data Center The core is the framework provider , which is the set of software modules that handles the abstraction of the complexity of the computer system and makes it visible as if it was a single entity. System Orchestrator -> manages application (both a human being or a program) ADD DETAILS FROM THE SLIDES!!! 'til 28 ADD DETAILS","title":"Big Data Architectures"},{"location":"big-data/infrastructures-architectures/architectures.html#analytical-applications","text":"Batch Analysis: Take a large amount of data and run analysis Run on demand Takes a lot of time Stream Analysis We devised an algorithm that runs continuously Real-time results Useful for monitoring Can we run both batch and stream on the same set of data? Lambda architecture Most trivial way to handle this problem. It consists in duplicating the data in order to apply both analysis. Data goes through two path: Hot path (timely, real-time, less accurate data) Cold path (less timely but more accurate data) Kappa architecture We can simplify the problem and handle everything a stream problem. Stream and batch application are different BUT we can run batch analysis on a stream engine. Data flows through a single patch, using a stream processing system. Lambda vs Kappa Lambda is easier but it needs parallel development and maintenance of two parallel pipelines. Kappa is the ongoing trend. A streaming engine can handle a bounded dataset and a well-designed streaming systems provide a strict superset of batch functionality.","title":"Analytical Applications"},{"location":"big-data/infrastructures-architectures/infrastructures.html","text":"Big Data Infrastructures Scaling -> big data does not fit into a single drive (or a single machine). Big data requires a lot of computer resources. SMP Architecture Symmetric Multi Processing, which is adopted by traditional RDBSM has physical limits regarding the number of devices that can be mounted, as well as a BUS bottleneck. Scaling Scale-up (adding more resources like processors, RAM, disk or upgrading the machine) Scale-out (adding more machines) MPP Architecture Massively Parallel Processing: Several processors, equipped with their own RAM and disks, collaborating to solve a single problem by splitting it in several interdependent tasks. This architecture requires specialized hardware. Vendor lock-in may be an issue with this architecture. Cluster Architecture (scale-out) A cluster is a group of linked computers (nodes), working together closely so that in many respects they form a single computer. There is no vendor lock-in Every node is a system on its own, capable of independent operations Unlimited scalability Compute nodes are stored on racks There can be many racks of computer nodes The nodes on a single rack are connected by a network Racks are connected by another level of network Scale-up vs Scale-out Scaling-up PROS: Lower power consumption and utility costs Less challenging to implement Lower licensing costs Specialized hardware and software Scaling-out PROS: Infinite scaling Generalist hardware and software Cheaper machines Usually cheaper overall Commodity Hardware Hardware that can be seen as a commodity Large range of vendors Multiple Clusters Having a single large cluster allows you to avoid data silos, leading to a simpler governance. However, multiple clusters are inevitable within medium-large enterprise settings: Resiliency (every cluster sits within a single point of failure) Software development (mitigate the risk of impacting critical production environments by isolating configuration, integration, or evolution testing and deployment) Workload isolation (hardware resources tuned for specific workloads) Legal separation Independent storage and compute With the success of cloud services, the independent storage and compute solution for big data clusters is on the rise DATA LOCALITY (locate the resource that deal with specific data near to that data, avoiding to move data from one machine to another) Machines that store data are usually up and running 24/7 as data must be persistent. Other distributed architectures Grid Computing - Similar to cluster computing - Each node is set to perform a different task High Performance Computing Massively parallel systems specifically developed to solve CPU-intensive tasks. Big data systems are mostly data-intensive.","title":"Big Data Infrastructure"},{"location":"big-data/infrastructures-architectures/infrastructures.html#big-data-infrastructures","text":"Scaling -> big data does not fit into a single drive (or a single machine). Big data requires a lot of computer resources.","title":"Big Data Infrastructures"},{"location":"big-data/infrastructures-architectures/infrastructures.html#smp-architecture","text":"Symmetric Multi Processing, which is adopted by traditional RDBSM has physical limits regarding the number of devices that can be mounted, as well as a BUS bottleneck.","title":"SMP Architecture"},{"location":"big-data/infrastructures-architectures/infrastructures.html#scaling","text":"Scale-up (adding more resources like processors, RAM, disk or upgrading the machine) Scale-out (adding more machines)","title":"Scaling"},{"location":"big-data/infrastructures-architectures/infrastructures.html#mpp-architecture","text":"Massively Parallel Processing: Several processors, equipped with their own RAM and disks, collaborating to solve a single problem by splitting it in several interdependent tasks. This architecture requires specialized hardware. Vendor lock-in may be an issue with this architecture.","title":"MPP Architecture"},{"location":"big-data/infrastructures-architectures/infrastructures.html#cluster-architecture-scale-out","text":"A cluster is a group of linked computers (nodes), working together closely so that in many respects they form a single computer. There is no vendor lock-in Every node is a system on its own, capable of independent operations Unlimited scalability Compute nodes are stored on racks There can be many racks of computer nodes The nodes on a single rack are connected by a network Racks are connected by another level of network","title":"Cluster Architecture (scale-out)"},{"location":"big-data/infrastructures-architectures/infrastructures.html#scale-up-vs-scale-out","text":"Scaling-up PROS: Lower power consumption and utility costs Less challenging to implement Lower licensing costs Specialized hardware and software Scaling-out PROS: Infinite scaling Generalist hardware and software Cheaper machines Usually cheaper overall Commodity Hardware Hardware that can be seen as a commodity Large range of vendors","title":"Scale-up vs Scale-out"},{"location":"big-data/infrastructures-architectures/infrastructures.html#multiple-clusters","text":"Having a single large cluster allows you to avoid data silos, leading to a simpler governance. However, multiple clusters are inevitable within medium-large enterprise settings: Resiliency (every cluster sits within a single point of failure) Software development (mitigate the risk of impacting critical production environments by isolating configuration, integration, or evolution testing and deployment) Workload isolation (hardware resources tuned for specific workloads) Legal separation Independent storage and compute With the success of cloud services, the independent storage and compute solution for big data clusters is on the rise DATA LOCALITY (locate the resource that deal with specific data near to that data, avoiding to move data from one machine to another) Machines that store data are usually up and running 24/7 as data must be persistent.","title":"Multiple Clusters"},{"location":"big-data/infrastructures-architectures/infrastructures.html#other-distributed-architectures","text":"Grid Computing - Similar to cluster computing - Each node is set to perform a different task High Performance Computing Massively parallel systems specifically developed to solve CPU-intensive tasks. Big data systems are mostly data-intensive.","title":"Other distributed architectures"},{"location":"big-data/introduction/SUMMARY.html","text":"Introduction to Big Data Big Data Lifecycle Processing Big Data Job Opportunities","title":"SUMMARY"},{"location":"big-data/introduction/data-processing.html","text":"Processing Big Data When you are on a distributed architecture, it is important to manage the distribution of machines. Many problems can happen in distributed environments. Since data is spread across machines, it is important to replicate those data. Also, it will not be possible to update data at the same time so you will need to deal with consistency (eventual consistency). it may happen that data collected at the same time may refer to different momentos. Big Data Software Stack New programming environments designed to get their parallelism not from a supercomputer but from computer clusters. Apache Hadoop automate the management of low level applications low-level tool, more advanced tools can be applied to it It is a software library (framework) that allows for the distributed processing of large data sets across clusters of computers using simple programming models. Rather than relying on hardware to deliver high-availability and reliability, the library itself is designed to detect and handle failures at the application layer, so as to deliver a highly-available service on top of a cluster of computers. Hadoop Modules HDFS (Hadoop Distributed File System) - storage layer that handles the storage of data across different machines provides ABSTRACTION on the storage of data YARN (Yet Another Resource Negotiator) - computation decides which unit of application runs in which machine if some unit of work fails in some machine, it will recreate it in another machine Map Reduce - analysis YARN-based system for parallel processing of large data sets On top of Hadoop Many different programming solutions can be applied on hadoop: Analytics (batch) simple/complex computations over large amounts of stored data Interactive (real-time) operational perspective Streaming (near-real-time) continuous analytics analysis run on continuously incoming data there is no much time and resources, you need to adopt some approximation algorithms Big Data Flow In the distributed file system, it is often used the metaphor of the Data Lake . A data lake is a central repository system for storage, processing, and analysis of raw data, in which the data is kept in its original format and is processed to be queried only when needed. It can store a varied amount of formats in big data ecosystems, from unstructured, semi-structured, to structured data sources. NoSQL - NewSQL DBMSs Relational DBMSs have not been designed to easily distributed. NoSQL DBMSs have risen to fill this gap. NewSQL is the latest frontier which combines the benefits from both relational and NoSQL worlds. Techniques for Big Data Analysis Extract, transform, and load (ETL) Data fusion and data integration Data management Analytics Data mining Association rule learning Classification Cluster analysis Regression Machine learning Supervised learning Unsupervised learning Cloud computing Goals of Analytics Descriptive Analytics (give insights into the past) Diagnostic Analytics (understand why something has happened) integrating the dataset analyzed with other data to look for correlation paths Predictive Analytics (look at the future) Prescriptive Analytics (prescribe what action to take to eliminate a future problem)","title":"Processing Big Data"},{"location":"big-data/introduction/data-processing.html#processing-big-data","text":"When you are on a distributed architecture, it is important to manage the distribution of machines. Many problems can happen in distributed environments. Since data is spread across machines, it is important to replicate those data. Also, it will not be possible to update data at the same time so you will need to deal with consistency (eventual consistency). it may happen that data collected at the same time may refer to different momentos.","title":"Processing Big Data"},{"location":"big-data/introduction/data-processing.html#big-data-software-stack","text":"New programming environments designed to get their parallelism not from a supercomputer but from computer clusters. Apache Hadoop automate the management of low level applications low-level tool, more advanced tools can be applied to it It is a software library (framework) that allows for the distributed processing of large data sets across clusters of computers using simple programming models. Rather than relying on hardware to deliver high-availability and reliability, the library itself is designed to detect and handle failures at the application layer, so as to deliver a highly-available service on top of a cluster of computers. Hadoop Modules HDFS (Hadoop Distributed File System) - storage layer that handles the storage of data across different machines provides ABSTRACTION on the storage of data YARN (Yet Another Resource Negotiator) - computation decides which unit of application runs in which machine if some unit of work fails in some machine, it will recreate it in another machine Map Reduce - analysis YARN-based system for parallel processing of large data sets On top of Hadoop Many different programming solutions can be applied on hadoop: Analytics (batch) simple/complex computations over large amounts of stored data Interactive (real-time) operational perspective Streaming (near-real-time) continuous analytics analysis run on continuously incoming data there is no much time and resources, you need to adopt some approximation algorithms","title":"Big Data Software Stack"},{"location":"big-data/introduction/data-processing.html#big-data-flow","text":"In the distributed file system, it is often used the metaphor of the Data Lake . A data lake is a central repository system for storage, processing, and analysis of raw data, in which the data is kept in its original format and is processed to be queried only when needed. It can store a varied amount of formats in big data ecosystems, from unstructured, semi-structured, to structured data sources.","title":"Big Data Flow"},{"location":"big-data/introduction/data-processing.html#nosql-newsql-dbmss","text":"Relational DBMSs have not been designed to easily distributed. NoSQL DBMSs have risen to fill this gap. NewSQL is the latest frontier which combines the benefits from both relational and NoSQL worlds.","title":"NoSQL - NewSQL DBMSs"},{"location":"big-data/introduction/data-processing.html#techniques-for-big-data-analysis","text":"Extract, transform, and load (ETL) Data fusion and data integration Data management Analytics Data mining Association rule learning Classification Cluster analysis Regression Machine learning Supervised learning Unsupervised learning Cloud computing","title":"Techniques for Big Data Analysis"},{"location":"big-data/introduction/data-processing.html#goals-of-analytics","text":"Descriptive Analytics (give insights into the past) Diagnostic Analytics (understand why something has happened) integrating the dataset analyzed with other data to look for correlation paths Predictive Analytics (look at the future) Prescriptive Analytics (prescribe what action to take to eliminate a future problem)","title":"Goals of Analytics"},{"location":"big-data/introduction/intro.html","text":"Big Data Nowadays, we produce more data than the capability of analyze them. Data grows faster than energy on chip Check out the updated infographic SKA telescope produce 3TB every second. They cannot store all the data but they need to analyze them and store the results. Big Data - Definition How can we distinguish big data from normal data? The line is quite vague Big data exceeds the reach of commonly used hardware environments and software tools to capture, manage, and process it with in a tolerable elapsed time for its user population - Teradata We can define Big Data following the V's principles: VOLUME (dataset that are particularly big) VELOCITY, interpreted in two ways: velocity in which data are injected speed of the analysis that you want to run VARIETY (many different formats of data) structured vs semi-structured (JSON) VERACITY (in many cases, you are dealing on datasets which you cannot fully rely on) especially true when you are dealing with social data The 'V' concept can be extended but we only consider the first four as they are the main ones. Big Data Hype Big Data comes mainly from two phenomenons: explosion of social networks IoT (sensors, smart cities, wearables, industry 4.0) The data that comes from these two sources is quite enormous with respect to the amount of data produced by companies. The Long Tail Model - Pareto Rule reversed The highest value does not come from the small set of highly popular items, but from the long list of niche items. Insignificant data is actually the most valuable. The possibility to handle large amount of data makes you smarter. Sometimes, no complex algorithm is needed: Google Translate just collects snippets of translations, match it with a long list of translations stored in their dataset, and return the most used one. The system is continuously debugged. Success Stories German National Football Team They applied data analysis to football and they won the 2016 world championship. Crime Prevention in LA Diagnosis and Treatment of Genetic Disease Investments in the Financial Sector Astronomical Discoveries Injury Prevention of Football Players Todays' Opportunities and Use Cases Healthcare (remote monitoring, preventive care, reduced hospitalization, improved system efficiency) Manufacturing (sensors) Location-Based Services Public Sector (citizen surveys) Retail (social media) Privacy Marketing campaigns are particularly effective when costumers are going through a change (maternity, new job, lifestyle). A company wanted to send advertising about maternity products BEFORE the baby was actually born. They were able to identify patterns of behaviors adopted by costumers when a baby was coming. However, the company incurred in legal issues because they sent maternity adv to a 16 y.o girl who did not inform the parents about the pregnancy. The company won the legal dispute but it raised some concerns regarding privacy.","title":"Introduction to Big Data"},{"location":"big-data/introduction/intro.html#big-data","text":"Nowadays, we produce more data than the capability of analyze them. Data grows faster than energy on chip Check out the updated infographic SKA telescope produce 3TB every second. They cannot store all the data but they need to analyze them and store the results.","title":"Big Data"},{"location":"big-data/introduction/intro.html#big-data-definition","text":"How can we distinguish big data from normal data? The line is quite vague Big data exceeds the reach of commonly used hardware environments and software tools to capture, manage, and process it with in a tolerable elapsed time for its user population - Teradata We can define Big Data following the V's principles: VOLUME (dataset that are particularly big) VELOCITY, interpreted in two ways: velocity in which data are injected speed of the analysis that you want to run VARIETY (many different formats of data) structured vs semi-structured (JSON) VERACITY (in many cases, you are dealing on datasets which you cannot fully rely on) especially true when you are dealing with social data The 'V' concept can be extended but we only consider the first four as they are the main ones.","title":"Big Data - Definition"},{"location":"big-data/introduction/intro.html#big-data-hype","text":"Big Data comes mainly from two phenomenons: explosion of social networks IoT (sensors, smart cities, wearables, industry 4.0) The data that comes from these two sources is quite enormous with respect to the amount of data produced by companies. The Long Tail Model - Pareto Rule reversed The highest value does not come from the small set of highly popular items, but from the long list of niche items. Insignificant data is actually the most valuable. The possibility to handle large amount of data makes you smarter. Sometimes, no complex algorithm is needed: Google Translate just collects snippets of translations, match it with a long list of translations stored in their dataset, and return the most used one. The system is continuously debugged.","title":"Big Data Hype"},{"location":"big-data/introduction/intro.html#success-stories","text":"German National Football Team They applied data analysis to football and they won the 2016 world championship. Crime Prevention in LA Diagnosis and Treatment of Genetic Disease Investments in the Financial Sector Astronomical Discoveries Injury Prevention of Football Players","title":"Success Stories"},{"location":"big-data/introduction/intro.html#todays-opportunities-and-use-cases","text":"Healthcare (remote monitoring, preventive care, reduced hospitalization, improved system efficiency) Manufacturing (sensors) Location-Based Services Public Sector (citizen surveys) Retail (social media)","title":"Todays' Opportunities and Use Cases"},{"location":"big-data/introduction/intro.html#privacy","text":"Marketing campaigns are particularly effective when costumers are going through a change (maternity, new job, lifestyle). A company wanted to send advertising about maternity products BEFORE the baby was actually born. They were able to identify patterns of behaviors adopted by costumers when a baby was coming. However, the company incurred in legal issues because they sent maternity adv to a 16 y.o girl who did not inform the parents about the pregnancy. The company won the legal dispute but it raised some concerns regarding privacy.","title":"Privacy"},{"location":"big-data/introduction/job-opportunities.html","text":"Job Opportunities Between 2019 and 2023, companies will hire 210K to 267K professionals with skills in mathematics, computer science, and 4.0. Most demanded professional figures: Data Scientist Big Data Analyst Cloud Computing Expert Cyber Security Expert Business Intelligence Analyst Social Media Marketing Manager Artificial Intelligence Systems Engineer Data Scientist The main figure emerged with big data is the one of the data scientist (sexiest job in the world). He deals with data analysis once data volume and velocity reaches a level requiring sophisticated technical skills. Data Architect Develop data architecture to effectively capture, integrate, organize, centralize and maintain data. Data Engineer Develop, test and maintain data architectures to keep data accessible and ready for analysis. Data Analyst Processes and interprets data to get actionable insights for a company.","title":"Job Opportunities"},{"location":"big-data/introduction/job-opportunities.html#job-opportunities","text":"Between 2019 and 2023, companies will hire 210K to 267K professionals with skills in mathematics, computer science, and 4.0. Most demanded professional figures: Data Scientist Big Data Analyst Cloud Computing Expert Cyber Security Expert Business Intelligence Analyst Social Media Marketing Manager Artificial Intelligence Systems Engineer","title":"Job Opportunities"},{"location":"big-data/introduction/job-opportunities.html#data-scientist","text":"The main figure emerged with big data is the one of the data scientist (sexiest job in the world). He deals with data analysis once data volume and velocity reaches a level requiring sophisticated technical skills.","title":"Data Scientist"},{"location":"big-data/introduction/job-opportunities.html#data-architect","text":"Develop data architecture to effectively capture, integrate, organize, centralize and maintain data.","title":"Data Architect"},{"location":"big-data/introduction/job-opportunities.html#data-engineer","text":"Develop, test and maintain data architectures to keep data accessible and ready for analysis.","title":"Data Engineer"},{"location":"big-data/introduction/job-opportunities.html#data-analyst","text":"Processes and interprets data to get actionable insights for a company.","title":"Data Analyst"},{"location":"big-data/introduction/life-cycle.html","text":"Big Data Lifecycle Acquisition Selection (understand which data is actually valuable) Filtering and Compression (very important because raw data is often too voluminous to store it all) Collect Metadata Collecting Metadata is fundamental to understand, measure, and control the data. Metadata describes the data so it enables trustworthiness, reproducibility and debugging. There are some software tools that allow you to collect metadata. This is a job that cannot be fully integrated. human input is required in this phase :sad: Extraction Depending on the analysis that you need to run, you work with specific data. Transformation and Normalization Cleaning and Error Handling Very important because of the untrustworthy of big data Integration In most cases, you will work with data coming from different sources, so, you will need to integrate them. The activities performed to integrate data: discover the relationship between datasets standardization, conflict management and entity resolution resolve heterogeneity and conflicts in data structure and semantics understand the trade-off of different modeling strategies Analysis Exploration (approach the data with new explorative approaches to gain a full understanding) Analytics (understand which approach works better to solve business problems) Delivery (find the best way to model and represent the results) Interpretation You need to be careful because sometimes it is common to rush to conclusions. It is important to verify the results: when you work with big data you should work on small artificial samples to verify expectations identify a subset of the data collected, analyze it and verify the results. It is interesting to remember that CORRELATION between data does not always mean that it is real. Correlation is not Causation :heart: Decision The decision-making process requires strong managerial skills","title":"Big Data Lifecycle"},{"location":"big-data/introduction/life-cycle.html#big-data-lifecycle","text":"","title":"Big Data Lifecycle"},{"location":"big-data/introduction/life-cycle.html#acquisition","text":"Selection (understand which data is actually valuable) Filtering and Compression (very important because raw data is often too voluminous to store it all) Collect Metadata Collecting Metadata is fundamental to understand, measure, and control the data. Metadata describes the data so it enables trustworthiness, reproducibility and debugging. There are some software tools that allow you to collect metadata. This is a job that cannot be fully integrated. human input is required in this phase :sad:","title":"Acquisition"},{"location":"big-data/introduction/life-cycle.html#extraction","text":"Depending on the analysis that you need to run, you work with specific data. Transformation and Normalization Cleaning and Error Handling Very important because of the untrustworthy of big data","title":"Extraction"},{"location":"big-data/introduction/life-cycle.html#integration","text":"In most cases, you will work with data coming from different sources, so, you will need to integrate them. The activities performed to integrate data: discover the relationship between datasets standardization, conflict management and entity resolution resolve heterogeneity and conflicts in data structure and semantics understand the trade-off of different modeling strategies","title":"Integration"},{"location":"big-data/introduction/life-cycle.html#analysis","text":"Exploration (approach the data with new explorative approaches to gain a full understanding) Analytics (understand which approach works better to solve business problems) Delivery (find the best way to model and represent the results)","title":"Analysis"},{"location":"big-data/introduction/life-cycle.html#interpretation","text":"You need to be careful because sometimes it is common to rush to conclusions. It is important to verify the results: when you work with big data you should work on small artificial samples to verify expectations identify a subset of the data collected, analyze it and verify the results. It is interesting to remember that CORRELATION between data does not always mean that it is real. Correlation is not Causation :heart:","title":"Interpretation"},{"location":"big-data/introduction/life-cycle.html#decision","text":"The decision-making process requires strong managerial skills","title":"Decision"}]}; var __search = { index: Promise.resolve(local_index) }