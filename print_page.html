
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.4.2">
    
    
      
        <title>Print as PDF - CescaNeri/BigData-CloudPlatforms</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.69437709.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.cbb835fc.min.css">
        
      
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="css/print-site-enum-headings1.css">
    
      <link rel="stylesheet" href="css/print-site-enum-headings2.css">
    
      <link rel="stylesheet" href="css/print-site-enum-headings3.css">
    
      <link rel="stylesheet" href="css/print-site.css">
    
      <link rel="stylesheet" href="css/print-site-material.css">
    
    <script>__md_scope=new URL("/",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#section-big-data-introduction" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="CescaNeri/BigData-CloudPlatforms" class="md-header__button md-logo" aria-label="CescaNeri/BigData-CloudPlatforms" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CescaNeri/BigData-CloudPlatforms
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print as PDF
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="teal"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="teal"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/CescaNeri/BigData-CloudPlatforms" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="CescaNeri/BigData-CloudPlatforms" class="md-nav__button md-logo" aria-label="CescaNeri/BigData-CloudPlatforms" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12Z"/></svg>

    </a>
    CescaNeri/BigData-CloudPlatforms
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/CescaNeri/BigData-CloudPlatforms" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1">
          Big Data Introduction
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Big Data Introduction" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          Big Data Introduction
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/introduction/intro.html" class="md-nav__link">
        Introduction to Big Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/introduction/life-cycle.html" class="md-nav__link">
        Big Data Lifecycle
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/introduction/data-processing.html" class="md-nav__link">
        Processing Big Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/introduction/job-opportunities.html" class="md-nav__link">
        Job Opportunities
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Infrastructure and Architecture
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Infrastructure and Architecture" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Infrastructure and Architecture
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/infrastructures-architectures/infrastructures.html" class="md-nav__link">
        Big Data Infrastructure
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/infrastructures-architectures/architectures.html" class="md-nav__link">
        Big Data Architecture
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Distributed File Systems
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Distributed File Systems" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Distributed File Systems
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/distributed-file-system/hadoop-dfs.html" class="md-nav__link">
        Hadoop DFS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/distributed-file-system/file-formats.html" class="md-nav__link">
        File Formats
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Batch Application
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Batch Application" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Batch Application
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/batch-application/yarn.html" class="md-nav__link">
        YARN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/batch-application/map-reduce.html" class="md-nav__link">
        Map Reduce
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/batch-application/apache-spark.html" class="md-nav__link">
        Apache Spark
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/batch-application/partitioning.html" class="md-nav__link">
        Partitioning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          SQL on Hadoop
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="SQL on Hadoop" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          SQL on Hadoop
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/sql-hadoop/spark-sql.html" class="md-nav__link">
        Spark SQL
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          NoSQL DBMSs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NoSQL DBMSs" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          NoSQL DBMSs
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/nosql-dbms/intro.html" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/nosql-dbms/consistency.html" class="md-nav__link">
        Managing Consistency
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          Data Streaming
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data Streaming" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Data Streaming
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/streaming/data-streaming.html" class="md-nav__link">
        Data Streaming Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/streaming/architecture.html" class="md-nav__link">
        Architecture
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="big-data/streaming/algorithms.html" class="md-nav__link">
        Algorithms
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" type="checkbox" id="__nav_8" >
      
      
      
      
        <label class="md-nav__link" for="__nav_8">
          Cloud Platforms Introduction
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Cloud Platforms Introduction" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          Cloud Platforms Introduction
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="cloud-platforms/introduction/intro.html" class="md-nav__link">
        From Databases to Data Platforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="cloud-platforms/introduction/data-management.html" class="md-nav__link">
        Data Management
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_9" type="checkbox" id="__nav_9" >
      
      
      
      
        <label class="md-nav__link" for="__nav_9">
          Cloud Computing
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Cloud Computing" data-md-level="1">
        <label class="md-nav__title" for="__nav_9">
          <span class="md-nav__icon md-icon"></span>
          Cloud Computing
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="cloud-platforms/cloud-computing/cloud-computing.html" class="md-nav__link">
        Cloud Computing
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_10" type="checkbox" id="__nav_10" >
      
      
      
      
        <label class="md-nav__link" for="__nav_10">
          AWS Lab
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="AWS Lab" data-md-level="1">
        <label class="md-nav__title" for="__nav_10">
          <span class="md-nav__icon md-icon"></span>
          AWS Lab
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="cloud-platforms/aws-lab/california-housing.html" class="md-nav__link">
        California House Pricing
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Print as PDF
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="print_page.html" class="md-nav__link md-nav__link--active">
        Print as PDF
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#section-big-data-introduction" class="md-nav__link">
    I. Big Data Introduction
  </a>
  
    <nav class="md-nav" aria-label="I. Big Data Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-introduction-intro" class="md-nav__link">
    1. Introduction to Big Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-introduction-life-cycle" class="md-nav__link">
    2. Big Data Lifecycle
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-introduction-data-processing" class="md-nav__link">
    3. Processing Big Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-introduction-job-opportunities" class="md-nav__link">
    4. Job Opportunities
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-infrastructure-and-architecture" class="md-nav__link">
    II. Infrastructure and Architecture
  </a>
  
    <nav class="md-nav" aria-label="II. Infrastructure and Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-infrastructures-architectures-infrastructures" class="md-nav__link">
    5. Big Data Infrastructure
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-infrastructures-architectures-architectures" class="md-nav__link">
    6. Big Data Architecture
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-distributed-file-systems" class="md-nav__link">
    III. Distributed File Systems
  </a>
  
    <nav class="md-nav" aria-label="III. Distributed File Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-distributed-file-system-hadoop-dfs" class="md-nav__link">
    7. Hadoop DFS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-distributed-file-system-file-formats" class="md-nav__link">
    8. File Formats
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-batch-application" class="md-nav__link">
    IV. Batch Application
  </a>
  
    <nav class="md-nav" aria-label="IV. Batch Application">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-batch-application-yarn" class="md-nav__link">
    9. YARN
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-batch-application-map-reduce" class="md-nav__link">
    10. Map Reduce
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-batch-application-apache-spark" class="md-nav__link">
    11. Apache Spark
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-batch-application-partitioning" class="md-nav__link">
    12. Partitioning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-sql-on-hadoop" class="md-nav__link">
    V. SQL on Hadoop
  </a>
  
    <nav class="md-nav" aria-label="V. SQL on Hadoop">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-sql-hadoop-spark-sql" class="md-nav__link">
    13. Spark SQL
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-nosql-dbmss" class="md-nav__link">
    VI. NoSQL DBMSs
  </a>
  
    <nav class="md-nav" aria-label="VI. NoSQL DBMSs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-nosql-dbms-intro" class="md-nav__link">
    14. Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-nosql-dbms-consistency" class="md-nav__link">
    15. Managing Consistency
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-streaming" class="md-nav__link">
    VII. Data Streaming
  </a>
  
    <nav class="md-nav" aria-label="VII. Data Streaming">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-streaming-data-streaming" class="md-nav__link">
    16. Data Streaming Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-streaming-architecture" class="md-nav__link">
    17. Architecture
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-streaming-algorithms" class="md-nav__link">
    18. Algorithms
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-cloud-platforms-introduction" class="md-nav__link">
    VIII. Cloud Platforms Introduction
  </a>
  
    <nav class="md-nav" aria-label="VIII. Cloud Platforms Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cloud-platforms-introduction-intro" class="md-nav__link">
    19. From Databases to Data Platforms
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cloud-platforms-introduction-data-management" class="md-nav__link">
    20. Data Management
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-cloud-computing" class="md-nav__link">
    IX. Cloud Computing
  </a>
  
    <nav class="md-nav" aria-label="IX. Cloud Computing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cloud-platforms-cloud-computing-cloud-computing" class="md-nav__link">
    21. Cloud Computing
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-aws-lab" class="md-nav__link">
    X. AWS Lab
  </a>
  
    <nav class="md-nav" aria-label="X. AWS Lab">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cloud-platforms-aws-lab-california-housing" class="md-nav__link">
    22. California House Pricing
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#section-big-data-introduction" class="md-nav__link">
    I. Big Data Introduction
  </a>
  
    <nav class="md-nav" aria-label="I. Big Data Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-introduction-intro" class="md-nav__link">
    1. Introduction to Big Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-introduction-life-cycle" class="md-nav__link">
    2. Big Data Lifecycle
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-introduction-data-processing" class="md-nav__link">
    3. Processing Big Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-introduction-job-opportunities" class="md-nav__link">
    4. Job Opportunities
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-infrastructure-and-architecture" class="md-nav__link">
    II. Infrastructure and Architecture
  </a>
  
    <nav class="md-nav" aria-label="II. Infrastructure and Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-infrastructures-architectures-infrastructures" class="md-nav__link">
    5. Big Data Infrastructure
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-infrastructures-architectures-architectures" class="md-nav__link">
    6. Big Data Architecture
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-distributed-file-systems" class="md-nav__link">
    III. Distributed File Systems
  </a>
  
    <nav class="md-nav" aria-label="III. Distributed File Systems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-distributed-file-system-hadoop-dfs" class="md-nav__link">
    7. Hadoop DFS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-distributed-file-system-file-formats" class="md-nav__link">
    8. File Formats
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-batch-application" class="md-nav__link">
    IV. Batch Application
  </a>
  
    <nav class="md-nav" aria-label="IV. Batch Application">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-batch-application-yarn" class="md-nav__link">
    9. YARN
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-batch-application-map-reduce" class="md-nav__link">
    10. Map Reduce
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-batch-application-apache-spark" class="md-nav__link">
    11. Apache Spark
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-batch-application-partitioning" class="md-nav__link">
    12. Partitioning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-sql-on-hadoop" class="md-nav__link">
    V. SQL on Hadoop
  </a>
  
    <nav class="md-nav" aria-label="V. SQL on Hadoop">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-sql-hadoop-spark-sql" class="md-nav__link">
    13. Spark SQL
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-nosql-dbmss" class="md-nav__link">
    VI. NoSQL DBMSs
  </a>
  
    <nav class="md-nav" aria-label="VI. NoSQL DBMSs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-nosql-dbms-intro" class="md-nav__link">
    14. Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-nosql-dbms-consistency" class="md-nav__link">
    15. Managing Consistency
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-streaming" class="md-nav__link">
    VII. Data Streaming
  </a>
  
    <nav class="md-nav" aria-label="VII. Data Streaming">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-data-streaming-data-streaming" class="md-nav__link">
    16. Data Streaming Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-streaming-architecture" class="md-nav__link">
    17. Architecture
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-data-streaming-algorithms" class="md-nav__link">
    18. Algorithms
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-cloud-platforms-introduction" class="md-nav__link">
    VIII. Cloud Platforms Introduction
  </a>
  
    <nav class="md-nav" aria-label="VIII. Cloud Platforms Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cloud-platforms-introduction-intro" class="md-nav__link">
    19. From Databases to Data Platforms
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cloud-platforms-introduction-data-management" class="md-nav__link">
    20. Data Management
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-cloud-computing" class="md-nav__link">
    IX. Cloud Computing
  </a>
  
    <nav class="md-nav" aria-label="IX. Cloud Computing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cloud-platforms-cloud-computing-cloud-computing" class="md-nav__link">
    21. Cloud Computing
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-aws-lab" class="md-nav__link">
    X. AWS Lab
  </a>
  
    <nav class="md-nav" aria-label="X. AWS Lab">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cloud-platforms-aws-lab-california-housing" class="md-nav__link">
    22. California House Pricing
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
  
                


<div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <div id="print-site-banner">
            <p>
    <em>This box will disappear when printing</em>
    <span style="float: right"><a href="https://timvink.github.io/mkdocs-print-site-plugin/">mkdocs-print-site-plugin</a></span>
</p>
<p>
    This page has combined all site pages into one. You can export to PDF using <b>File > Print > Save as PDF</b>.
</p>
<p>
    See also <a href="https://timvink.github.io/mkdocs-print-site-plugin/how-to/export-PDF.html">export to PDF</a> and <a href="https://timvink.github.io/mkdocs-print-site-plugin/how-to/export-HTML.html">export to standalone HTML</a>.
</p>
        </div>
        
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Index</h1>
                </nav>
            </div>
        </section>
        
                        <h1 class='nav-section-title' id='section-big-data-introduction'>
                            Big Data Introduction <a class='headerlink' href='#section-big-data-introduction' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="big-data-introduction-intro"><h1 id="big-data-introduction-intro-big-data">Big Data</h1>
<p>Nowadays, we produce more data than the capability of analyze them.</p>
<ul>
<li>Data grows faster than energy on chip</li>
</ul>
<p><a href="https://www.domo.com/learn/infographic/data-never-sleeps-9">Check out the updated infographic</a></p>
<p>SKA telescope produce 3TB every second. 
They cannot store all the data but they need to analyze them and store the results.</p>
<p><img alt="" src="big-data/introduction/domo.jpg" /></p>
<h1 id="big-data-introduction-intro-big-data-definition">Big Data - Definition</h1>
<p>How can we distinguish big data from normal data?
The line is quite vague</p>
<blockquote>
<p>Big data exceeds the reach of commonly used hardware environments and software tools to capture, manage, and process it with in a tolerable elapsed time for its user population
<em>- Teradata</em></p>
</blockquote>
<p>We can define Big Data following the V's principles:</p>
<ul>
<li>VOLUME (dataset that are particularly big)</li>
<li>VELOCITY, interpreted in two ways:<ul>
<li>velocity in which data are injected </li>
<li>speed of the analysis that you want to run</li>
</ul>
</li>
<li>VARIETY (many different formats of data)<ul>
<li>structured vs semi-structured (JSON)</li>
</ul>
</li>
<li>VERACITY (in many cases, you are dealing on datasets which you cannot fully rely on)<ul>
<li>especially true when you are dealing with social data </li>
</ul>
</li>
</ul>
<p><img alt="" src="big-data/introduction/datav.jpg" /></p>
<p>The 'V' concept can be extended but we only consider the first four as they are the main ones.</p>
<h2 id="big-data-introduction-intro-big-data-hype">Big Data Hype</h2>
<p>Big Data comes mainly from two phenomenons:</p>
<ul>
<li>explosion of social networks</li>
<li>IoT (sensors, smart cities, wearables, industry 4.0)</li>
</ul>
<p>The data that comes from these two sources is quite enormous with respect to the amount of data produced by companies. </p>
<p><strong>The Long Tail Model</strong> - Pareto Rule <em>reversed</em></p>
<p>The highest value does not come from the small set of highly popular items, but from the long list of niche items.</p>
<ul>
<li>Insignificant data is actually the most valuable.</li>
</ul>
<p>The possibility to handle large amount of data makes you smarter. Sometimes, no complex algorithm is needed:</p>
<p>Google Translate just collects snippets of translations, match it with a long list of translations stored in their dataset, and return the most used one.</p>
<ul>
<li>The system is continuously debugged.</li>
</ul>
<h2 id="big-data-introduction-intro-success-stories">Success Stories</h2>
<ul>
<li>German National Football Team</li>
</ul>
<p>They applied data analysis to football and they won the 2016 world championship.</p>
<ul>
<li>Crime Prevention in LA</li>
<li>Diagnosis and Treatment of Genetic Disease </li>
<li>Investments in the Financial Sector</li>
<li>Astronomical Discoveries</li>
<li>Injury Prevention of Football Players</li>
</ul>
<h2 id="big-data-introduction-intro-todays-opportunities-and-use-cases">Todays' Opportunities and Use Cases</h2>
<ol>
<li>Healthcare (remote monitoring, preventive care, reduced hospitalization, improved system efficiency)</li>
<li>Manufacturing (sensors)</li>
<li>Location-Based Services</li>
<li>Public Sector (citizen surveys)</li>
<li>Retail (social media)</li>
</ol>
<h2 id="big-data-introduction-intro-privacy">Privacy</h2>
<p>Marketing campaigns are particularly effective when costumers are going through a change (maternity, new job, lifestyle).
A company wanted to send advertising about maternity products BEFORE the baby was actually born. They were able to identify patterns of behaviors adopted by costumers when a baby was coming. 
However, the company incurred in legal issues because they sent maternity adv to a 16 y.o girl who did not inform the parents about the pregnancy.
The company won the legal dispute but it raised some concerns regarding privacy. </p></section><section class="print-page" id="big-data-introduction-life-cycle"><h1 id="big-data-introduction-life-cycle-big-data-lifecycle">Big Data Lifecycle</h1>
<p><img alt="" src="big-data/introduction/lifecycle.jpg" /></p>
<h2 id="big-data-introduction-life-cycle-acquisition">Acquisition</h2>
<ul>
<li>Selection (understand which data is actually valuable)</li>
<li>Filtering and Compression (very important because raw data is often too voluminous to store it all)</li>
<li>Collect <strong>Metadata</strong>
Collecting Metadata is fundamental to understand, measure, and control the data. Metadata describes the data so it enables trustworthiness, reproducibility and debugging.</li>
</ul>
<p>There are some software tools that allow you to collect metadata. This is a job that cannot be fully integrated. </p>
<ul>
<li>human input is required in this phase :sad:</li>
</ul>
<h2 id="big-data-introduction-life-cycle-extraction">Extraction</h2>
<p>Depending on the analysis that you need to run, you work with specific data.</p>
<ul>
<li>Transformation and Normalization</li>
<li>Cleaning and Error Handling<ul>
<li>Very important because of the untrustworthy of big data</li>
</ul>
</li>
</ul>
<h2 id="big-data-introduction-life-cycle-integration">Integration</h2>
<p>In most cases, you will work with data coming from different sources, so, you will need to integrate them. </p>
<p>The activities performed to integrate data:</p>
<ul>
<li>discover the relationship between datasets</li>
<li>standardization, conflict management and entity resolution<ul>
<li>resolve heterogeneity and conflicts in data structure and semantics</li>
<li>understand the trade-off of different modeling strategies</li>
</ul>
</li>
</ul>
<h2 id="big-data-introduction-life-cycle-analysis">Analysis</h2>
<ul>
<li>Exploration (approach the data with new explorative approaches to gain a full understanding)</li>
<li>Analytics (understand which approach works better to solve business problems)</li>
<li>Delivery (find the best way to model and represent the results)</li>
</ul>
<h2 id="big-data-introduction-life-cycle-interpretation">Interpretation</h2>
<p>You need to be careful because sometimes it is common to rush to conclusions.
It is important to verify the results:</p>
<ul>
<li>when you work with big data you should work on small artificial samples to verify expectations</li>
<li>identify a subset of the data collected, analyze it and verify the results.</li>
</ul>
<p>It is interesting to remember that CORRELATION between data does not always mean that it is real.</p>
<blockquote>
<p>Correlation is not Causation :heart:</p>
</blockquote>
<h2 id="big-data-introduction-life-cycle-decision">Decision</h2>
<p>The decision-making process requires strong managerial skills </p></section><section class="print-page" id="big-data-introduction-data-processing"><h1 id="big-data-introduction-data-processing-processing-big-data">Processing Big Data</h1>
<p>When you are on a distributed architecture, it is important to manage the distribution of machines.
Many problems can happen in distributed environments. </p>
<p>Since data is spread across machines, it is important to replicate those data. 
Also, it will not be possible to update data at the same time so you will need to deal with consistency (eventual consistency).</p>
<ul>
<li>it may happen that data collected at the same time may refer to different momentos.</li>
</ul>
<h2 id="big-data-introduction-data-processing-big-data-software-stack">Big Data Software Stack</h2>
<p>New programming environments designed to get their parallelism not from a supercomputer but from computer clusters.</p>
<p><strong>Apache Hadoop</strong></p>
<ul>
<li>automate the management of low level applications</li>
<li>low-level tool, more advanced tools can be applied to it</li>
</ul>
<p>It is a software library (framework) that allows for the distributed processing of large data sets across clusters of computers using simple programming models.</p>
<p>Rather than relying on hardware to deliver high-availability and reliability, the library itself is designed to detect and handle failures at the application layer, so as to deliver a highly-available service on top of a cluster of computers.</p>
<p><strong>Hadoop Modules</strong></p>
<ul>
<li>HDFS (Hadoop Distributed File System) - storage<ul>
<li>layer that handles the storage of data across different machines</li>
<li>provides ABSTRACTION on the storage of data</li>
</ul>
</li>
<li>YARN (Yet Another Resource Negotiator) - computation<ul>
<li>decides which unit of application runs in which machine</li>
<li>if some unit of work fails in some machine, it will recreate it in another machine</li>
</ul>
</li>
<li>Map Reduce - analysis<ul>
<li>YARN-based system for parallel processing of large data sets</li>
</ul>
</li>
</ul>
<p><strong>On top of Hadoop</strong></p>
<p>Many different programming solutions can be applied on hadoop:</p>
<ol>
<li>Analytics (batch)<ul>
<li>simple/complex computations over large amounts of stored data</li>
</ul>
</li>
<li>Interactive (real-time)<ul>
<li>operational perspective </li>
</ul>
</li>
<li>Streaming (near-real-time)<ul>
<li>continuous analytics</li>
<li>analysis run on continuously incoming data </li>
<li>there is no much time and resources, you need to adopt some approximation algorithms </li>
</ul>
</li>
</ol>
<h2 id="big-data-introduction-data-processing-big-data-flow">Big Data Flow</h2>
<p><img alt="" src="big-data/introduction/data-flow.jpg" /></p>
<p>In the distributed file system, it is often used the metaphor of the <strong>Data Lake</strong>.
A data lake is a central repository system for storage, processing, and analysis of raw data, in which the data is kept in its original format and is processed to be queried only when needed. 
It can store a varied amount of formats in big data  ecosystems, from unstructured, semi-structured, to structured data sources.  </p>
<h2 id="big-data-introduction-data-processing-nosql-newsql-dbmss">NoSQL - NewSQL DBMSs</h2>
<p>Relational DBMSs have not been designed to easily distributed. NoSQL DBMSs have risen to fill this gap.
NewSQL is the latest frontier which combines the benefits from both relational and NoSQL worlds.</p>
<h2 id="big-data-introduction-data-processing-techniques-for-big-data-analysis">Techniques for Big Data Analysis</h2>
<ul>
<li>Extract, transform, and load (ETL)</li>
<li>Data fusion and data integration</li>
<li>Data management</li>
<li>Analytics<ul>
<li>Data mining<ul>
<li>Association rule learning</li>
<li>Classification</li>
<li>Cluster analysis</li>
<li>Regression</li>
</ul>
</li>
<li>Machine learning<ul>
<li>Supervised learning</li>
<li>Unsupervised learning</li>
</ul>
</li>
</ul>
</li>
<li>Cloud computing</li>
</ul>
<h2 id="big-data-introduction-data-processing-goals-of-analytics">Goals of Analytics</h2>
<ul>
<li>Descriptive Analytics (give insights into the past)</li>
<li>Diagnostic Analytics (understand why something has happened)<ul>
<li>integrating the dataset analyzed with other data to look for correlation paths</li>
</ul>
</li>
<li>Predictive Analytics (look at the future)</li>
<li>Prescriptive Analytics (prescribe what action to take to eliminate a future problem)</li>
</ul></section><section class="print-page" id="big-data-introduction-job-opportunities"><h1 id="big-data-introduction-job-opportunities-job-opportunities">Job Opportunities</h1>
<p>Between 2019 and 2023, companies will hire 210K to 267K professionals with skills in mathematics, computer science, and 4.0.
Most demanded professional figures:</p>
<ul>
<li>Data Scientist</li>
<li>Big Data Analyst</li>
<li>Cloud Computing Expert</li>
<li>Cyber Security Expert</li>
<li>Business Intelligence Analyst</li>
<li>Social Media Marketing Manager</li>
<li>Artificial Intelligence Systems Engineer</li>
</ul>
<h2 id="big-data-introduction-job-opportunities-data-scientist">Data Scientist</h2>
<p>The main figure emerged with big data is the one of the data scientist (sexiest job in the world).</p>
<p>He deals with data analysis once data volume and velocity reaches a level requiring sophisticated technical skills.</p>
<p><img alt="" src="big-data/introduction/data-science.jpg" /></p>
<h2 id="big-data-introduction-job-opportunities-data-architect">Data Architect</h2>
<p>Develop data architecture to effectively capture, integrate, organize, centralize and maintain data.</p>
<h2 id="big-data-introduction-job-opportunities-data-engineer">Data Engineer</h2>
<p>Develop, test and maintain data architectures to keep data accessible and ready for analysis.</p>
<h2 id="big-data-introduction-job-opportunities-data-analyst">Data Analyst</h2>
<p>Processes and interprets data to get actionable insights for a company.</p></section><h1 class='nav-section-title-end'>Ended: Big Data Introduction</h1>
                        <h1 class='nav-section-title' id='section-infrastructure-and-architecture'>
                            Infrastructure and Architecture <a class='headerlink' href='#section-infrastructure-and-architecture' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="big-data-infrastructures-architectures-infrastructures"><h1 id="big-data-infrastructures-architectures-infrastructures-big-data-infrastructures">Big Data Infrastructures</h1>
<p><strong>Scaling</strong> -&gt; big data does not fit into a single drive (or a single machine). 
Big data requires a lot of computer resources.</p>
<h2 id="big-data-infrastructures-architectures-infrastructures-smp-architecture">SMP Architecture</h2>
<p>Symmetric Multi Processing, which is adopted by traditional RDBSM has physical limits regarding the number of devices that can be mounted, as well as a BUS bottleneck.</p>
<h2 id="big-data-infrastructures-architectures-infrastructures-scaling">Scaling</h2>
<ul>
<li>Scale-up (adding more resources like processors, RAM, disk or upgrading the machine)</li>
<li>Scale-out (adding more machines)</li>
</ul>
<h2 id="big-data-infrastructures-architectures-infrastructures-mpp-architecture">MPP Architecture</h2>
<p>Massively Parallel Processing:</p>
<ul>
<li>Several processors, equipped with their own RAM and disks, collaborating to solve a single problem by splitting it in several interdependent tasks.</li>
<li>This architecture requires specialized hardware. </li>
<li>Vendor lock-in may be an issue with this architecture.</li>
</ul>
<h2 id="big-data-infrastructures-architectures-infrastructures-cluster-architecture-scale-out">Cluster Architecture (scale-out)</h2>
<p>A cluster is a group of linked computers (nodes), working together closely so that in many respects they form a single computer.</p>
<ul>
<li>There is no vendor lock-in</li>
<li>Every node is a system on its own, capable of independent operations </li>
<li>Unlimited scalability </li>
</ul>
<p>Compute nodes are stored on racks</p>
<ul>
<li>There can be many racks of computer nodes</li>
<li>The nodes on a single rack are connected by a network</li>
<li>Racks are connected by another level of network</li>
</ul>
<h2 id="big-data-infrastructures-architectures-infrastructures-scale-up-vs-scale-out">Scale-up vs Scale-out</h2>
<p>Scaling-up PROS:</p>
<ul>
<li>Lower power consumption and utility costs</li>
<li>Less challenging to implement</li>
<li>Lower licensing costs</li>
<li>Specialized hardware and software</li>
</ul>
<p>Scaling-out PROS:</p>
<ul>
<li>Infinite scaling</li>
<li>Generalist hardware and software</li>
<li>Cheaper machines</li>
<li>Usually cheaper overall</li>
<li>Commodity Hardware<ul>
<li>Hardware that can be seen as a commodity</li>
<li>Large range of vendors</li>
</ul>
</li>
</ul>
<h2 id="big-data-infrastructures-architectures-infrastructures-multiple-clusters">Multiple Clusters</h2>
<p>Having a single large cluster allows you to avoid data silos, leading to a simpler governance.</p>
<p>However, multiple clusters are inevitable within medium-large enterprise settings:</p>
<ul>
<li>Resiliency (every cluster sits within a single point of failure)</li>
<li>Software development (mitigate the risk of impacting critical production environments by isolating configuration, integration, or evolution testing and deployment)</li>
<li>Workload isolation (hardware resources tuned for specific workloads)</li>
<li>Legal separation</li>
<li>Independent storage and compute</li>
</ul>
<p>With the success of cloud services, the independent storage and compute solution for big data clusters is on the rise</p>
<ul>
<li>DATA LOCALITY (locate the resource that deal with specific data near to that data, avoiding to move data from one machine to another)</li>
</ul>
<p>Machines that store data are usually up and running 24/7 as data must be persistent. </p>
<h2 id="big-data-infrastructures-architectures-infrastructures-other-distributed-architectures">Other distributed architectures</h2>
<p><strong>Grid Computing</strong>
- Similar to cluster computing
- Each node is set to perform a different task</p>
<p><strong>High Performance Computing</strong>
Massively parallel systems specifically developed to solve CPU-intensive tasks.
Big data systems are mostly data-intensive.</p>
<p><img alt="" src="big-data/infrastructures-architectures/hpc.jpg" /></p></section><section class="print-page" id="big-data-infrastructures-architectures-architectures"><h1 id="big-data-infrastructures-architectures-architectures-big-data-architectures">Big Data Architectures</h1>
<p>There are a lot of problems related to distributed architectures, especially concerning parallelization:</p>
<ul>
<li>Communication between workers</li>
<li>Access to shared resources</li>
<li>Parallelization and concurrency</li>
</ul>
<p>The solution is to have single computer -&gt; <strong>Data Center</strong></p>
<p><img alt="" src="big-data/infrastructures-architectures/datacenter.jpg" /></p>
<p>The core is the <strong>framework provider</strong>, which is the set of software modules that handles the abstraction of the complexity of the computer system and makes it visible as if it was a single entity.</p>
<p><strong>System Orchestrator</strong> -&gt; manages application (both a human being or a program)</p>
<p>ADD DETAILS FROM THE SLIDES!!! 'til 28</p>
<p><img alt="" src="big-data/infrastructures-architectures/arch.jpg" /></p>
<p>ADD DETAILS</p>
<h2 id="big-data-infrastructures-architectures-architectures-analytical-applications">Analytical Applications</h2>
<ul>
<li><strong>Batch Analysis:</strong> Take a large amount of data and run analysis<ul>
<li>Run on demand</li>
<li>Takes a lot of time</li>
</ul>
</li>
<li><strong>Stream Analysis</strong> We devised an algorithm that runs continuously <ul>
<li>Real-time results</li>
<li>Useful for monitoring</li>
</ul>
</li>
</ul>
<p>Can we run both batch and stream on the same set of data?</p>
<p><strong>Lambda architecture</strong>
Most trivial way to handle this problem. It consists in duplicating the data in order to apply both analysis.
Data goes through two path:</p>
<ul>
<li>Hot path (timely, real-time, less accurate data)</li>
<li>Cold path (less timely but more accurate data)</li>
</ul>
<p><strong>Kappa architecture</strong>
We can simplify the problem and handle everything a stream problem.
Stream and batch application are different BUT we can run batch analysis on a stream engine.
Data flows through a single patch, using a stream processing system. </p>
<p><strong>Lambda vs Kappa</strong>
Lambda is easier but it needs parallel development and maintenance of two parallel pipelines. 
Kappa is the ongoing trend. A streaming engine can handle a bounded dataset and a well-designed streaming systems provide a strict superset of batch functionality.</p></section><h1 class='nav-section-title-end'>Ended: Infrastructure and Architecture</h1>
                        <h1 class='nav-section-title' id='section-distributed-file-systems'>
                            Distributed File Systems <a class='headerlink' href='#section-distributed-file-systems' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="big-data-distributed-file-system-hadoop-dfs"><h1 id="big-data-distributed-file-system-hadoop-dfs-hadoop-distributed-file-system">Hadoop Distributed File System</h1>
<p>The first thing that software has to provide is <strong>abstraction</strong>, as we want to interact with the disks of the machines as if it was a single machine.
We want a single reference and we want to talk to a single entity.</p>
<ul>
<li>Master-Slave architecture 
A single entity works as a master and handles the storage of the data with a set of slaves that are running of single machines.</li>
</ul>
<p>The software needs to deal with <strong>big data</strong>:</p>
<ul>
<li>Files may be bigger than single disks</li>
<li>We need to split files into smaller blocks and store them on different machines</li>
</ul>
<p>ALso, <strong>fault tolerance</strong> is of key importance: disks can fail, machines can be unreachable, but data should always be available.</p>
<ul>
<li>we can store multiple copies of each block</li>
</ul>
<h2 id="big-data-distributed-file-system-hadoop-dfs-hdfs-definition">HDFS - definition</h2>
<p>HDFS is filesystem designed for storing very large files with a streaming data access patterns, running on clusters of commodity hardware.</p>
<ul>
<li>Application that run on HDFS need streaming access to their data sets.<ul>
<li>I do not want to continuously update data with batch operations.</li>
<li>The emphasis is on high throughput of data access rather than low latency of data access</li>
</ul>
</li>
</ul>
<p><strong>Blocks</strong> -&gt; splitting files into block that range between 64MB and 1GB.
We need blocks because files can be larger than disks.</p>
<ul>
<li>Why are blocks this big?
We focus on giving a high throughput as large files split into many small blocks require a huge number of seeks. </li>
</ul>
<h2 id="big-data-distributed-file-system-hadoop-dfs-master-slave-abstraction">Master - Slave Abstraction</h2>
<p>Master service -&gt; <em>namenode</em></p>
<ul>
<li>Persistently maintains the filesystem tree</li>
<li>Coordinates the storage on different machines</li>
<li>Keeps in memory the location of each block for a given file (<strong>block pool</strong>)
Slaves -&gt; <em>datanodes</em></li>
<li>Store and retrieve blocks</li>
</ul>
<h2 id="big-data-distributed-file-system-hadoop-dfs-spof">SPoF</h2>
<p>The namenode is a single point of failure: without it, the file system cannot be used.</p>
<ul>
<li><strong>Backup solution</strong>: not the best solution</li>
<li><strong>Secondary NN Solution</strong>: separate machine in which a secondary namenode is installed and does some synchronization with the primary one (while being in a sleeping state).</li>
</ul>
<h2 id="big-data-distributed-file-system-hadoop-dfs-high-availability">High Availability</h2>
<p>Most of the big data tools and frameworks use this term to reference solutions that answer to the problem of always having data available no matter what.</p>
<p>HA indicates a system that can tolerate faults.</p>
<ul>
<li>Supported by configuring two separate machines as NNs:<ul>
<li>One is active (up and running)</li>
<li>The other is in standby
In the event of failure of the active NN, the standby NN takes over.</li>
</ul>
</li>
</ul>
<p>It s a more complex solution in terms of resources and communication with the network, but it guarantees that in case of failure, there will not be any downtime.</p>
<h2 id="big-data-distributed-file-system-hadoop-dfs-hdfs-federation">HDFS - federation</h2>
<p>There is no need to have a single file system, but we can have many systems on the same cluster.
We can create multiple NN that manage different file systems.</p>
<p>This has several advantages:</p>
<ul>
<li><strong>Performance</strong> (if we have a large filesystem with lots of data, the NN can be a bottleneck in terms of answering to multiple requests of users. By dividing file systems with different NN that address different requests, performance will increase)</li>
<li>Availability</li>
<li><strong>Scalability</strong> (when the file system is large, the metadata can become quite huge. The metadata block needs to be lowered in memory so splitting metadata among different machines allows scalability)</li>
<li>Maintainability, Security and Flexibility</li>
</ul>
<h2 id="big-data-distributed-file-system-hadoop-dfs-hdfs-replication">HDFS - replication</h2>
<p>Whenever we want to save a file in the file system, this will be splitted in blocks and stored separately.</p>
<p>Each data block is independently replicated at multiple DNs in order to improve performance and robustness. </p>
<p>Nodes are organized in racks, that are organized in data centers.</p>
<ul>
<li>Hadoop models such concepts in a tree-liked fashion and computes the distance between nodes as their distance on the tree.</li>
</ul>
<p>The typical rule is to store the first <strong>replica</strong> on the node (n1) where the client issued the write command.
Replica 2 is stores on a node (n2) in a rack (r2) different form n1.
Replica 3 is stored on a node different from n2 but tha belongs to r2.</p>
<p><img alt="" src="big-data/distributed-file-system/racks.jpg" /></p>
<p><strong>Hadoop 3</strong> alternative to simple replication:</p>
<ul>
<li>Each block is split across each data node</li>
<li>It reduces data redundancy</li>
<li>It requires less storage (less money)</li>
<li>Faster writes (when we need to write data, the data to write are less)</li>
<li>DISADVANTAGES:<ul>
<li>Higher CPU cost (whenever we want to access data, having split the blocks in further blocks, we need to rebuild the blocks)</li>
<li>Loss of data locality (whenever we have a unit of work that is running on a certain machine, the system tries to instatiate this application as close to the data as possible. This works if all the data are in the same machine, but in this solution data are slit in different machines. Therefore, we cannot apply the data locality principle.)</li>
<li>Longer recovery time </li>
</ul>
</li>
</ul>
<h2 id="big-data-distributed-file-system-hadoop-dfs-hdfs-not-always-the-best-fit">HDFS not always the best fit</h2>
<p>Although this may change in the future, there are area where HDFS is not a good fit:</p>
<ol>
<li>Low-latency data access</li>
<li>Lots of small files</li>
</ol>
<p><strong>I/O communication</strong></p>
<p>An application client wishing to read a file must first contact the NN to determine where the actual data is stored.</p>
<ul>
<li>The NN identifies the relevant block</li>
<li>The client contacts the DN to retrieve the data</li>
</ul>
<p><strong>Features of design:</strong></p>
<ul>
<li>The namenode never removes data</li>
<li>All data transfer occurs directly between clients and DNs</li>
<li>Communications with the NN only involve transfer of metadata</li>
</ul></section><section class="print-page" id="big-data-distributed-file-system-file-formats"><h1 id="big-data-distributed-file-system-file-formats-file-formats">File Formats</h1>
<p>The hadoop ecosystem supports several different formats:</p>
<ul>
<li>Standard file formats</li>
<li>Hadoop / BigData-specific formats</li>
</ul>
<p>Hadoop-specific formats involves several advantages:</p>
<ul>
<li><strong>Serialization</strong></li>
<li><strong>Splittability</strong> (if the file is split into multiple blocks, metadata headers allow to skip unnecessary I/O)<ul>
<li>Metadata is associated with each single block (the client can access individual blocks)</li>
</ul>
</li>
<li><strong>Compression</strong> (if you want to compress the data, you use a compression mechanism that works on a block-level or at a record-level)</li>
</ul>
<p>There are many file formats - <strong>row-oriented</strong>:</p>
<ul>
<li>Sequence files</li>
<li>Apache thrift (Facebook), Protocol buffers (Google)<ul>
<li>They both create a notion of schema that has to be used in order to read the data</li>
<li>Some code specifies how the data are structured</li>
</ul>
</li>
<li>Apache Avro <ul>
<li>Each file contains its own schema definition (no need to share separately the schema of the file because it is included in the file itself)</li>
</ul>
</li>
</ul>
<h2 id="big-data-distributed-file-system-file-formats-column-oriented-file-formats">Column-oriented file formats</h2>
<p>This formats store data based on columns.
Column-oriented formats are better suited for analytical scenarios. This because in a typical analytical query, you want to take data from a subset of columns and perform analysis.</p>
<p>This format is not ideal for operational purposes (day-to-day operations in a database).</p>
<p>There are several advantaged to column-oriented file formats:</p>
<ul>
<li>Better compression (similar values because data is more homogeneous)</li>
<li>Reduced I/O for analytical queries</li>
<li>Operate on encoded data</li>
</ul>
<p>There are many file formats:</p>
<ul>
<li>ORC Files</li>
<li>Apache Parquet (general purpose)</li>
</ul>
<h2 id="big-data-distributed-file-system-file-formats-parquet">Parquet</h2>
<p>Besides storing data in a column format, it allows to store <strong>nested structures</strong> (like JSON) in a flat format.</p>
<p><strong>Data Model</strong>
 Nested attributes that have multiple values.</p>
<ul>
<li>Types:<ul>
<li>Group</li>
<li>Primitive</li>
</ul>
</li>
<li>Frequency:<ul>
<li>Required</li>
<li>Optional</li>
<li>Repeated</li>
</ul>
</li>
</ul>
<p><img alt="" src="big-data/distributed-file-system/nested.jpg" /></p>
<p><strong>Unnesting</strong>
Can we store nested data structures in a columnar format?
we need to map the schema to a list of columns in a way that we can write records to flat columns and read them back to their original nested data structure.</p>
<ul>
<li>Each value is associated with two integers (repetition level and definition level)</li>
<li>These integers allow to fully reconstruct the nested structures while still being able to store each primitive separately</li>
</ul>
<p>When I put values in a column formats, I do not know which value belongs to which record. 
Rebuilding the structure of the rows just by looking at the values cannot be done.
The parquet format allows to reconstruct the message.</p>
<p><img alt="" src="big-data/distributed-file-system/parquet.jpg" /></p>
<p>Parquet uses a definition level that defines the level of definition (0,1,2,3...), dealing with optionality.
This allows you to understand which values exist.
Inside the hierarchy, some fields are mandatory and others are optional. </p>
<p><strong>Repetition Level</strong>
The presence of repeated fields requires to store when new lists are starting in a column of values.
Repetition level indicates at which level we have to create a new list for the current value. </p>
<p>By using these two values (repetition and definition), I am able to fully reconstruct the message.</p>
<p>Repetition and definition level cause overhead (they occupy space).</p>
<ul>
<li>The number of bits that has to be used is quite small (1,2,3... the number of levels) and in some case they can be omitted </li>
<li>The repetition levels are stored in a column so they can be compressed </li>
</ul>
<p><strong>Parquet file format</strong>
We have a notion of <em>row group</em> -&gt; the data are not fully columnar. We do not store all the values of each column continuously.
Usually, we split the file into row groups (horizontal partitioning) and each block of rows store the data in a columnar way. </p>
<ul>
<li><strong>COlumn chunk</strong> (chunk of the data for a particular column)
All the values of each columns is further subdivided into pages.
The size of the row group is set to the same size of the block (each block corresponds to a row).
<strong>Data page size</strong> -&gt; it can be tuned (smaller or larger) depending on what query we need to issue.</li>
</ul></section><h1 class='nav-section-title-end'>Ended: Distributed File Systems</h1>
                        <h1 class='nav-section-title' id='section-batch-application'>
                            Batch Application <a class='headerlink' href='#section-batch-application' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="big-data-batch-application-yarn"><h1 id="big-data-batch-application-yarn-yarn-yet-another-resource-negotiator">YARN - Yet ANother Resource Negotiator</h1>
<p>In the same cluster, I want to run some programs and applications. The problem regards how these computations allocate resources.</p>
<p>The resource negotiator:</p>
<ul>
<li>In charge of assigning units of work</li>
<li>Has a global view of the resources  available in the cluster</li>
<li>Decides where and when each uint of work should be allocated and with how many resources</li>
<li>Uses a scheduling policy to manage concurrency</li>
<li>Avoids over-instantiation of processes </li>
<li>Handles fault-tolerance</li>
</ul>
<p>Potentially, we can run as many process as we want on each machine but we do not want to overload it.</p>
<p>YARN provides APIs for requesting and working with cluster resources.</p>
<ul>
<li>It is a general framework (works with any kind of application you want to run)</li>
<li>Application are written using analytical frameworks</li>
</ul>
<h2 id="big-data-batch-application-yarn-main-daemons">Main Daemons</h2>
<p>YARN work with the master/slave mechanism:</p>
<ul>
<li>Resource Manager (ultimate authority that arbitrates among all the applications)</li>
<li>Node Manager (per-node slave)<ul>
<li>In charge of running containers which run applications (virtual abstract entities associated with a certain amount of resources. We have a virtual environment that run applications)</li>
</ul>
</li>
</ul>
<p>For each application. there is one container that runs a special process that in charge of coordinating the single operation.</p>
<p><strong>Coordination</strong> happens on two levels:</p>
<ol>
<li>Resource Manager, composed by two components:<ul>
<li>Application Manager (start the application - accept request from client and start the machine)</li>
<li>Scheduling component (decides where resources should be allocated)</li>
</ul>
</li>
<li>For each application, we have another process (<strong>application master process</strong>) that manages the resources of each application</li>
</ol>
<p>Application execution consists of the following steps:</p>
<ol>
<li>A client program submits the application, including the necessary specifications to launch the application-specific AMP itself</li>
<li>The RM assumes the responsibility to negotiate a specified container in which to start the AMP and then launches it</li>
<li>The AMP registers with the RM</li>
<li>The AMP negotiates appropriate resources containers</li>
<li>On successful container allocations, the AMP launches the container by providing the container launch specification</li>
<li>The application code executing within the container provides necessary information</li>
<li>During the application execution, the client that submitted the program communicates directly with the AMP to get status and updates</li>
<li>Once the application is complete, the AMP de-registers with the RM and shuts down, allowing its own container to be repurposed</li>
</ol>
<h2 id="big-data-batch-application-yarn-yarn-scheduler">YARN Scheduler</h2>
<p>YARN provides a choice of schedulers and configurable policies:</p>
<ul>
<li>FIFO scheduler (useless)</li>
<li>Fair Scheduler (with multiple applications that want to access the resource, the scheduler will balance the resources among the two applications)<ul>
<li>Problem related to prediction of execution time as resources are split</li>
</ul>
</li>
<li>Capacity Scheduler (the amount of resources is divided in two profiles - fixed amount)<ul>
<li>Each application run within a profile have access to a fixed amount of resources</li>
<li>Execution time is predictable but there may be a waste of resources</li>
</ul>
</li>
</ul>
<h2 id="big-data-batch-application-yarn-data-locality">Data Locality</h2>
<p>When the scheduler needs to identify the resources to allocate, it does so following the data locality principle.
The point is to exploit cluster typology and data block replication to apply the data locality principle. </p>
<p><em>When computation involves large set of data, it is cheaper to move code to data rather than data to code</em></p></section><section class="print-page" id="big-data-batch-application-map-reduce"><h1 id="big-data-batch-application-map-reduce-map-reduce">Map Reduce</h1>
<p>It is based, in general, on key-value pairs (data is structured in couples). It is a programming model and an associated implementation for processing and generating large data sets.</p>
<p>How it works:</p>
<p>It is based on typical analytical problems, in which you have large datasets and you want to extract something, you reorganize the data to compute aggregations and generate a final output.</p>
<p><strong>Map operations and reduce operations</strong></p>
<ul>
<li>MAP takes a function f and applies it to every element in a list</li>
<li>FOLD iteratively applies a function g to <strong>aggregate</strong> results</li>
</ul>
<h2 id="big-data-batch-application-map-reduce-parallelization">Parallelization</h2>
<p>The map operation includes all operations can be parallelized in a straightforward manner, since each functional application happens in isolation.</p>
<p>Reduce operation has more restrictions on data locality.</p>
<h2 id="big-data-batch-application-map-reduce-mapreduce-program">MapReduce Program</h2>
<p>The map function requires an input (key-value pairs), that are chosen by the programmer.</p>
<ul>
<li>Map (k1, v1) -&gt; list(k2, v2)</li>
<li>Reduce  (k2, list(v2)) -&gt; list(k3, v3)</li>
</ul>
<p>The output of the map function is a list containing all the values in the row, while the reduce function takes as input a key and the list with all the values associated with that key.</p>
<p>MapReduce program = <strong>job</strong></p>
<ul>
<li>Each job is divided into smaller units called <strong>tasks</strong></li>
<li>The tasks are scheduled using YARN and run on nodes in the cluster</li>
</ul>
<h2 id="big-data-batch-application-map-reduce-mapreduce-process">MapReduce Process</h2>
<ol>
<li>Input is divided into fixed-size splits</li>
<li>A Map task is created for each split</li>
<li>The key-value pairs returned by Map tasks are sorted and stored in the local disk</li>
<li>Map outputs are sent to the nodes where the Reduce tasks are running</li>
<li>The key-value pairs returned by Reduce tasks are written persistently onto the DFS</li>
</ol>
<p><img alt="" src="big-data/batch-application/mapreduce.jpg" /></p>
<h2 id="big-data-batch-application-map-reduce-example-word-count">Example - Word Count</h2>
<p>Counting the  number of occurrences for each word in a collection of documents.</p>
<p><strong>Input:</strong> a repository of documents (each document is a value in the input pairs)</p>
<p><strong>Map function:</strong> read a document and emit a sequence of key-value pairs</p>
<p><strong>Shuffle and Sort:</strong> group by key and generate a pairs of the form</p>
<p><strong>Reduce function:</strong> add up all the values for a given key and emits a pair of the form (w, m)</p>
<p><strong>Output:</strong> w is a word that appears at least once among all the input documents; m is the total number of occurrences of w among all those documents</p>
<div class="highlight"><pre><span></span><code><span class="n">Map</span><span class="p">(</span><span class="n">String</span><span class="w"> </span><span class="n">docid</span><span class="p">,</span><span class="w"> </span><span class="n">String</span><span class="w"> </span><span class="n">text</span><span class="p">):</span><span class="w"></span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">text</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="n">Emit</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"></span>

<span class="n">Reduce</span><span class="p">(</span><span class="n">String</span><span class="w"> </span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">counts</span><span class="o">[]</span><span class="p">):</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">counts</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">c</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">Emit</span><span class="p">(</span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">sum</span><span class="p">);</span><span class="w"></span>
</code></pre></div>
<h2 id="big-data-batch-application-map-reduce-map-in-java">Map in Java</h2>
<div class="highlight"><pre><span></span><code><span class="kd">public</span><span class="w"> </span><span class="kd">class</span> <span class="nc">WordCountMapperextendsMapper</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="p">,</span><span class="w"> </span><span class="n">Text</span><span class="p">,</span><span class="w"> </span><span class="n">Text</span><span class="p">,</span><span class="w"> </span><span class="n">IntWritable</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kd">private</span><span class="w"> </span><span class="kd">static</span><span class="w"> </span><span class="kd">final</span><span class="w"> </span><span class="n">IntWritableone</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">newIntWritable</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">privateText</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">newText</span><span class="p">();</span><span class="w"></span>
<span class="w">    </span><span class="kd">public</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">map</span><span class="p">(</span><span class="n">LongWritablekey</span><span class="p">,</span><span class="w"> </span><span class="n">Text</span><span class="w"> </span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="n">Context</span><span class="w"> </span><span class="n">context</span><span class="p">)</span><span class="w"> </span>
<span class="w">        </span><span class="n">throwsIOException</span><span class="p">,</span><span class="w"> </span><span class="n">InterruptedException</span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">String</span><span class="w"> </span><span class="n">line</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">value</span><span class="p">.</span><span class="na">toString</span><span class="p">();</span><span class="w"></span>
<span class="w">    </span><span class="n">StringTokenizertokenizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">newStringTokenizer</span><span class="p">(</span><span class="n">line</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">while</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="na">hasMoreTokens</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">word</span><span class="p">.</span><span class="na">set</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="na">nextToken</span><span class="p">());</span><span class="w"></span>
<span class="w">        </span><span class="n">context</span><span class="p">.</span><span class="na">write</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">one</span><span class="p">);</span><span class="w"></span>
<span class="w">        </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</code></pre></div>
<h2 id="big-data-batch-application-map-reduce-reduce-in-java">Reduce in Java</h2>
<div class="highlight"><pre><span></span><code><span class="kd">public</span><span class="w"> </span><span class="kd">class</span> <span class="nc">WordCountReducerextendsReducer</span><span class="o">&lt;</span><span class="n">Text</span><span class="p">,</span><span class="w"> </span><span class="n">IntWritable</span><span class="p">,</span><span class="w"> </span><span class="n">Text</span><span class="p">,</span><span class="w"> </span><span class="n">IntWritable</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kd">public</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">reduce</span><span class="p">(</span><span class="n">Text</span><span class="w"> </span><span class="n">key</span><span class="p">,</span><span class="w"> </span><span class="n">Iterable</span><span class="o">&lt;</span><span class="n">IntWritable</span><span class="o">&gt;</span><span class="w"> </span><span class="n">values</span><span class="p">,</span><span class="w"> </span><span class="n">Context</span><span class="w"> </span><span class="n">context</span><span class="p">)</span><span class="w">     </span>
<span class="w">        </span><span class="n">throwsIOException</span><span class="p">,</span><span class="w"> </span><span class="n">InterruptedException</span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">intsum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">for</span><span class="p">(</span><span class="n">IntWritablevalue</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">values</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">value</span><span class="p">.</span><span class="na">get</span><span class="p">();</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="n">context</span><span class="p">.</span><span class="na">write</span><span class="p">(</span><span class="n">key</span><span class="p">,</span><span class="w"> </span><span class="n">newIntWritable</span><span class="p">(</span><span class="n">sum</span><span class="p">));</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</code></pre></div>
<h2 id="big-data-batch-application-map-reduce-combiners">Combiners</h2>
<p>When the reduce function is associative and commutative, we can push some of what the reducers do to the Map tasks. 
In this case, we also apply a combiner to the Map function. The combiner function must be <strong>associative</strong> and <strong>commutative.</strong></p>
<p>Advantages:</p>
<ul>
<li>It reduces the amount of intermediate data</li>
<li>It reduces the network traffic</li>
</ul>
<h2 id="big-data-batch-application-map-reduce-data-partitioning">Data Partitioning</h2>
<p><strong>Maps:</strong></p>
<ul>
<li>Partitioning depends on the input splits</li>
<li>One map task per input split</li>
</ul>
<p><strong>Reducers:</strong></p>
<ul>
<li>Data is shuffled and according to partitioning function, it decides, for each key, which reducers it goes to</li>
<li>Based on the number of nodes and available resources</li>
<li>Can be defined by the user</li>
</ul>
<p>The keyspace of the intermediate key-value pairs is evenly distributed over the reducers with a hash function (same keys in different mappers end up at the same reducer).
The partitioning of the intermediate key-value pairs generated by the maps can be customized.</p>
<p><strong>How does it work:</strong></p>
<ul>
<li>Let p be the number of reduce tasks</li>
<li>The partitioner adopts a hash function that translates a key to a number from 0 to p-1</li>
<li>Each output ket-value pair is stored in one of p files</li>
<li>The reduce task collects from every map task the partitions with the same hash value</li>
</ul>
<p><strong>Tuning the number of reduce tasks</strong></p>
<p>Hadoop used to create only one, global reduce task by default (mainly used).
Deciding the number of reduce task is not easy; it is more an art than a science.</p>
<p>One solution would be to devise single task for every CPU available in the cluster. In this way, all CPUs are working but in reality, there are often more keys to process than available CPUs.</p>
<p>Another solution would be to create multiple tasks for each CPU. From a certain perspective, it may seem a waste but actually, this solution mitigate skewness problems.</p>
<p>General rule of thumb: each task should run for about 5 minutes and produce more than 1 HDFS block's worth of output.</p>
<p><img alt="" src="big-data/batch-application/map2.jpg" /></p>
<h2 id="big-data-batch-application-map-reduce-mapreduce-execution">MapReduce Execution</h2>
<p>An important idea behind MapReduce is separating the <em>what</em> of distributed processing from the <em>how.</em></p>
<p>The developer launches the job on the client's JMV, which contacts the YARN RM to submit the application.</p>
<p><strong>Data locality enforcement</strong>
Main principle: do not move data to workers, move workers to data.</p>
<p>The task are created from the input splits in the shared file system</p>
<p><strong>Optimization:</strong> prefer nodes that are on the same rack in the data center as the on holding the data block. Inter-rack bandwidth is significantly less than intra-rack bandwidth.</p>
<h2 id="big-data-batch-application-map-reduce-map-execution-steps">Map Execution STEPS</h2>
<p><img alt="" src="big-data/batch-application/map-1.jpg" /></p>
<p><img alt="" src="big-data/batch-application/map-2.jpg" /></p>
<p><img alt="" src="big-data/batch-application/map-3.jpg" /></p>
<p><img alt="" src="big-data/batch-application/map-4.jpg" /></p>
<p><img alt="" src="big-data/batch-application/map-5.jpg" /></p>
<h2 id="big-data-batch-application-map-reduce-mapreduce-algorithms">MapReduce Algorithms</h2>
<p>MapREduce is a framework, not a tool. You must fit your solution into the framework of map and reduce. In some situations, it might be challenging and translating ML and DM to the MapReduce paradigm is not trivial.</p>
<p>Sometimes, we may need multiple map / reduce stages and build chains of maps and reduces.</p>
<p><strong>Filtering algorithms</strong> to find lines/files/tuples with a particular characteristic.</p>
<p><strong>Summarization algorithms</strong> to compute the maximum/sum/average over a set of values.</p>
<p><strong>Join</strong> to combine different inputs on some shared values.</p>
<p><strong>Sort</strong> to sort inputs in the preferred order.</p>
<h2 id="big-data-batch-application-map-reduce-two-stage-mapreduce">Two Stage MapReduce</h2>
<p>As map-reduce calculations get more complex, it is useful to break them down into stages:</p>
<ul>
<li>The output of the first stage serves as input to the next one</li>
<li>The same output may be useful for different subsequent stages</li>
<li>The output can be stored in the DFS, forming a materialized view</li>
</ul>
<p>Early stages of map-reduce operations often represents the heaviest amount of data access, so building and saving them once as a basis for many downstream uses saves us a lot of work.</p></section><section class="print-page" id="big-data-batch-application-apache-spark"><h1 id="big-data-batch-application-apache-spark-apache-spark">Apache Spark</h1>
<p><em>How did we get from Hadoop's MapReduce to Spark?</em></p>
<p>Hadoop's MapReduce is the result of many years of development but in the meanwhile, a lot has changed.
Before, disk was the primary source of data and machines were mostly single core.
Now, RAM is the primary source of data and machines are mostly multi core.</p>
<p><strong>Limitations of MapReduce:</strong></p>
<ul>
<li>It is designed for batch processing</li>
<li>It has a strict paradigm</li>
<li>New hardware capabilities are not exploited</li>
<li>Too much complex </li>
</ul>
<h2 id="big-data-batch-application-apache-spark-apache-spark-framework">Apache Spark Framework</h2>
<p>Spark relies on two main abstractions:</p>
<ul>
<li>
<p><strong>RDD (Resilient Distributed Dataset)</strong>: immutable distributed collection of objects</p>
<ul>
<li><strong>Resilient</strong> (automatically rebuild on failure)</li>
<li><strong>Distributed</strong> (the objects belonging to a given collection are split into partitions and spread across the nodes)</li>
<li><strong>Immutable</strong> (once created, it cannot be modified)</li>
<li><strong>Lazily evaluated</strong> (optimization before execution)</li>
<li><strong>Cachable</strong> (it can persist in memory, spill to disk if necessary)</li>
<li><strong>Type inference</strong> (data types are not declared but inferred)</li>
</ul>
</li>
<li>
<p><strong>DAG (Direct Acyclic Graph)</strong>: based on the user application and on the lineage graphs. Sparks computes a logical execution plan in the form of a DAG.</p>
<ul>
<li>Sequence of computations performed on data</li>
<li>Nodes are the RDD (created to compute the result)</li>
<li>Edges represents the operations
The execution plan is compiled into physical stages</li>
</ul>
</li>
</ul>
<p>The distinction among different stages (boundaries) is set by shuffle operations. 
Operation with narrow dependencies are pipelined as much as possible.</p>
<p>For each stage, we have different <strong>tasks</strong>:</p>
<ul>
<li>A task is created for each partition in the new RDD</li>
<li>Tasks are scheduled and assigned to the worker nodes based on data locality</li>
<li>The scheduler can run the same task on multiple nodes in case of stragglers</li>
</ul>
<h2 id="big-data-batch-application-apache-spark-rdd">RDD</h2>
<p><strong>Creation</strong></p>
<p>RDD can be created in two ways:</p>
<ol>
<li>By loading an external dataset</li>
<li>By distributing a collection of objects that you have in memory</li>
</ol>
<p>By default, RDD are not persisted: they are recomputed each time they are needed in an action.
Therefore, there is no need to occupy memory if the RDD is not used again. </p>
<p><strong>Operations</strong></p>
<p>RDDs offer two types of operations:</p>
<ol>
<li>Transformations (all the operations that allow you to create a new RDD from a previous one)</li>
<li>Actions (compute a result that is either returned to the driver program or saved to an external storage system)</li>
</ol>
<p><strong>Metadata</strong></p>
<p>RDD transformations cause only metadata change. Each RDD stores a set of a metadata:</p>
<ul>
<li>Partitions (set of data splits associated with the RDD)</li>
<li>Partitioner (how the data is split into partitions)</li>
<li>Dependencies (list of parent RDDs involved in computation)</li>
<li>Compute (function to compute partition of the RDD, given the parent partitions from the dependencies)</li>
<li>Preferred locations (where is the best place to put computations on this partition)</li>
</ul>
<p><strong>Lineage</strong></p>
<p>The set of RDD dependencies is logically represented as a linear graph which allows for fault tolerance system without checkpointing.
The derived physical execution plan can be optimized by aggregating different operations and exploiting data locality.</p>
<p><strong>Dependencies</strong></p>
<p>In the dependency graph, we can distinguish between two kinds of dependencies:</p>
<ol>
<li>Narrow dependencies (it allows for pipelining on one cluster node)</li>
<li>Wide dependencies  (it cannot be pipelined, as they require data shuffling)</li>
</ol>
<p><img alt="" src="big-data/batch-application/dependencies.jpg" /></p>
<p>In spark we have different operations (Map function) that can be <em>reduced</em> using operations like 'groupbykey', 'reducebykey' and 'join'.</p>
<p><strong>Persistence</strong></p>
<p>Persisting a dataset in memory across operations is one of Spark's most important capabilities.
By default, each RDD is recomputed each time an action is run on it. When you persist an RDD, each node stored in memory the partitions that are computed, allowing for faster future actions.</p>
<h2 id="big-data-batch-application-apache-spark-spark-application-decomposition">Spark Application Decomposition</h2>
<ul>
<li>Application (single instance of SparkContext that stores data processing logic and schedules series of jobs, sequentially or in parallel)</li>
<li>Job (complete set of transformations on RDD that finishes with action or data saving, triggered by the driver application)</li>
<li>Stage (set of transformations that can be pipelined and executed by a single independent worker)</li>
<li>Task (basic unit of scheduling which executes the stages on a single data partition)</li>
</ul>
<p><img alt="" src="big-data/batch-application/spark-appl.jpg" /></p>
<h2 id="big-data-batch-application-apache-spark-spark-architecture">Spark Architecture</h2>
<p>Spark uses a <em>master/slave</em> architecture with one central coordinator (<em>driver</em>) and many distributed workers (<em>executors</em>).</p>
<p>Drivers and executers are independent Java processes which form a spark application.
The architecture is independent of the cluster manager that Spark runs on.</p>
<p><img alt="" src="big-data/batch-application/spark-arch.jpg" /></p>
<p><strong>Cluster Manager</strong>: it is responsible for assigning and managing the cluster's resources</p>
<p><strong>Executor</strong>: it is responsible for executing the received tasks (defined by the driver).
Each executor can run multiple tasks at the same time.</p>
<p><strong>Driver Program</strong>: each spark application have one driver that converts user program into tasks.</p>
<p><strong>Spark Architecture in YARN</strong></p>
<ul>
<li>Driver Program = Application</li>
<li>Executor &lt; Container</li>
<li>Cluster Manager = Resource Manager</li>
</ul>
<p><strong>Spark vs MapReduce</strong></p>
<p><img alt="" src="big-data/batch-application/spark-vs-map.jpg" /></p>
<p>Since executors are fixed, the containers to be instantiated are just those for the executors. 
Each container runs the executor and each executor can run multiple tasks.</p>
<p>The level of parallelization is restricted by the number of executors. </p>
<h2 id="big-data-batch-application-apache-spark-deployment">Deployment</h2>
<p>There are three different deployment modes:</p>
<ol>
<li><strong>Cluster mode</strong> (the driver process runs directly on a node in the cluster)</li>
<li><strong>Client mode</strong> (the driver process runs on a machine that does not belong to the cluster)</li>
<li><strong>Local mode</strong> (both driver and executors run on the same machine)</li>
</ol></section><section class="print-page" id="big-data-batch-application-partitioning"><h1 id="big-data-batch-application-partitioning-partitioning">Partitioning</h1>
<p>Data in RDD are split into multiple partitions, which serve two purposes:</p>
<ol>
<li>parallelize computation across workers and CPU cores</li>
<li>Minimize network traffic for data exchange between executors</li>
</ol>
<p>If not specified, SPark sets the number of partitions automatically. The number of partitions are important as one task is run for each partition:</p>
<p>Too few partitions results in less concurrency and risks from data skewness, while too many partitions results in excessive overhead in managing small tasks.</p>
<p><strong>Rule of thumb:</strong></p>
<ul>
<li>Lower bound: 2 partitions for each core</li>
<li>Upper bounds:<ul>
<li>each task should take at least 100/200 ms to execute</li>
<li>a block worth of data</li>
</ul>
</li>
</ul>
<p>When in doubt, more partitions is usually better.</p>
<h2 id="big-data-batch-application-partitioning-rdd-partitioning-criteria">RDD Partitioning Criteria</h2>
<p>The partitioning criteria is used to:</p>
<ul>
<li>Put values that belong to the same key in the same partition</li>
<li>Define how the key should be distributed in different partitions</li>
<li>No control over the specific worker node a partition goes to</li>
</ul>
<p>Partition criteria triggers shuffle operations so, forcing a partitioning criteria on a RDD is costly and there is no explicit control over which worker node each key goes on.</p>
<p><strong>Different partitioning data:</strong></p>
<ol>
<li><strong>Hash Partitioning:</strong> hashing using the Java hashCode</li>
<li><strong>Range Partitioning:</strong> distributes data into uniform ranges</li>
<li><strong>Custom Partitioning:</strong> user-defined partitioning</li>
</ol>
<p><strong>When is it useful?</strong></p>
<p>If I repartition an RDD based on some criteria, then I know that all the values with the same key will end up in the same partition.</p>
<h2 id="big-data-batch-application-partitioning-shuffling">Shuffling</h2>
<p>Shuffling is the mechanism used to re-distribute data across partitions. It is necessary to compute some operations but it is costly and complex.</p>
<p><img alt="" src="big-data/batch-application/hash.jpg" /></p>
<p><img alt="" src="big-data/batch-application/sort.jpg" /></p>
<p><img alt="" src="big-data/batch-application/tungsten.jpg" /></p>
<p>In time, Spark has provided a number of shuffling implementations:</p>
<ul>
<li><strong>Hash shuffle:</strong> each map task creates a file for every reducer. <a href="https://medium.com/@philipp.brunenberg/understanding-apache-spark-hash-shuffle-b9aed2d587b0">In this article is explained clearly (kinda)</a><ul>
<li>Each executor holds a pool of files</li>
<li>A single group contains a file for every reducer</li>
<li>An executor contains as many groups as the mapper than can run in parallel in the executor itself</li>
</ul>
</li>
<li><strong>Sort shuffle:</strong> each mapper keeps output in memory, spills to disk if necessary.<ul>
<li>Each mapper spills to its own file</li>
<li>Each file is sorted by reducer</li>
<li>When a reducer ask for data, the pieces from each file are collected, sorted in memory, and sent to the reducer</li>
</ul>
</li>
<li><strong>Tungsten sort:</strong> evolution of the sort shuffle, but it works on serialized data (no need to do de-serialization and serialization).<ul>
<li>Improves performance of the sort shuffle technique</li>
<li>It works only under certain constraints</li>
</ul>
</li>
</ul>
<p>As of today, one single implementation is provided: <em>SortShuffleManager</em></p>
<ul>
<li>If conditions allow it, tungsten sort is adopted</li>
<li>Hash shuffle is convenient only if you need to create an output with few partitions (low number of files)</li>
<li>Otherwise, sort shuffle is adopted</li>
</ul>
<h2 id="big-data-batch-application-partitioning-cluster-configuration">Cluster Configuration</h2>
<p>In the SPark architecture, the level of parallelization is also given by the number of resources allocated.</p>
<p>The number of resources that we can ask for is given in the amount of memory and CPU.
All the executors have the same configuration and the number of executors is fixed as well.</p>
<p><strong>Tuning CPU</strong></p>
<p>We can either set the number of <strong>executors</strong> or the cumber of <strong>cores</strong> per executors.
The best thing to do is to have fewer executor with a high number of cores. Single-core executors throw away the benefits that come from running multiple tasks in a single JVM.</p>
<p>When deciding the number of CPU, it is important to remember that not everything should be allocated to the executor, as there are other parties.</p>
<p>It is good practice to leave at least one core per machine for other services.</p>
<p><img alt="" src="big-data/batch-application/cpu.jpg" /></p>
<p><strong>Tuning Memory</strong></p>
<p>Tuning memory can be done by setting the amount of memory per executor.</p>
<p>We should keep in mind that executors with too much memory often result in excessive garbage collection delays.</p>
<p><img alt="" src="big-data/batch-application/memory.jpg" /></p></section><h1 class='nav-section-title-end'>Ended: Batch Application</h1>
                        <h1 class='nav-section-title' id='section-sql-on-hadoop'>
                            SQL on Hadoop <a class='headerlink' href='#section-sql-on-hadoop' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="big-data-sql-hadoop-spark-sql"><h1 id="big-data-sql-hadoop-spark-sql-sql-on-hadoop">SQL on Hadoop</h1>
<p>SQL on Hadoop is a class of application tools that combine established SQL-style querying with newer Hadoop data framework elements</p>
<p><strong>SQL options on Hadoop</strong></p>
<ol>
<li>Batch SQL (apache Hive)</li>
<li>In-Memory SQL (Apache Spark)</li>
<li>Interactive SQL (Impala)</li>
<li>Operational SQL (Apache HBase, NoSQL)</li>
</ol>
<p><strong>Data storage</strong></p>
<ul>
<li>Only NoSQL database directly handle data storage</li>
<li>Some tools provide a relational abstraction over HDFS</li>
</ul>
<p><strong>Execution Engine</strong></p>
<ul>
<li>Most tools directly handle query execution</li>
<li>Hive pushes execution to other tools</li>
</ul>
<h2 id="big-data-sql-hadoop-spark-sql-spark-sql">Spark SQL</h2>
<p>Instead of working with RDDs, we work with <strong>datasets</strong>.
With dataset, type conformity is checked at compile time.</p>
<p>They are <strong>lazily evaluated</strong> but supports under-the-hoop optimizations and code generation.</p>
<p><img alt="" src="big-data/sql-hadoop/spark.jpg" /></p>
<p><strong>Pros and Cons of Structure</strong></p>
<ul>
<li>Structure imposes some <strong>limits</strong> (RDDS enable any computation through user defined functions)</li>
<li>The most common computations are supported</li>
<li>Language simplicity</li>
<li>Opens the room to <strong>optimization</strong></li>
</ul>
<p><img alt="" src="big-data/sql-hadoop/sql.jpg" /></p>
<p>The goal is to obtain an efficient and optimized logical plan, on which physical plans are generated.</p>
<h2 id="big-data-sql-hadoop-spark-sql-logical-optimization">Logical Optimization</h2>
<p>It is based on <strong>rules</strong>, which is a function that can be applied on a portion of the logical plan.</p>
<p>Several types of rules:</p>
<ul>
<li>Constant folding (resolve constant expressions at compile time)</li>
<li>Predicate push-down (push selection predicates (filters) close to the sources)</li>
<li>Column pruning (project only the required column)</li>
<li>Join reordering (change the order of join operations)</li>
</ul>
<p>These rules are applied recursively (maybe the application of a rule enables the application of another and so on) until the plan reaches a <em>fixed point</em> and no more optimization can be done.</p>
<h2 id="big-data-sql-hadoop-spark-sql-physical-optimization">Physical Optimization</h2>
<p>Once you have a physical plan, you also need to estimate the execution time.
The cost of operating on a certain dataset is characterized by the amount of time lost in terms of computation, CPU, I/O operations from the disk.</p>
<ul>
<li>I/O operations depends on the size of the data set</li>
<li>The cost of the amount of computation depends on the cardinality of the dataset</li>
</ul>
<p><img alt="" src="big-data/sql-hadoop/cost.jpg" /></p>
<p>Other important factors:</p>
<ul>
<li>Network throughput</li>
<li>Disk throughput</li>
<li>Allocation of resources</li>
<li>Allocation of tasks</li>
</ul>
<p><strong>Rule based physical optimization</strong></p>
<ul>
<li>Operation pipelining</li>
<li>Predicate push-down (filtering performed during physical scan)</li>
</ul>
<p><strong>Join method selection</strong></p>
<ul>
<li><strong>Broadcast Hash</strong> join (if you have two dataset to join and one is not very big, you can replicate the same table for all the partition of the other dataset to avoid reorganization)</li>
<li><strong>Shuffle Hash</strong> join (data are reorganized based on the join attribute and the HJ method is applied)</li>
</ul>
<p>To select a join method, the following conditions need to be considered:</p>
<p><img alt="" src="big-data/sql-hadoop/conditions.jpg" /></p>
<h2 id="big-data-sql-hadoop-spark-sql-adaptive-query-execution">Adaptive Query Execution</h2>
<p>Every time that you need to carry out a shuffle operation, the <strong>catalyst</strong> process is repeated again, because there are many parameters that have to be checked.</p>
<p><img alt="" src="big-data/sql-hadoop/catalyst.jpg" /></p>
<p>The main idea of AQE is to do reviews at each stage boundary.</p>
<p>If we want to carry out a join operation to identify the sales carried out in NY.
The sales dataset is partitioned by the stores, so we have multiple partitions (blocks) depending on the store_id.</p>
<p>WIthout optimization we would need to carry out the join on all the stores and then apply a filter on NY.</p>
<p>With AQE, we can partition data through broadcast hash join.</p>
<p><img alt="" src="big-data/sql-hadoop/dynamic.jpg" /></p>
<p><strong>Drawbacks</strong></p>
<ul>
<li>The execution stops at each stage boundary for SPark to review its plan but the gain in performance in usually worth</li>
<li>The Spark UI is more difficult to read as each stage becomes a different job</li>
</ul></section><h1 class='nav-section-title-end'>Ended: SQL on Hadoop</h1>
                        <h1 class='nav-section-title' id='section-nosql-dbmss'>
                            NoSQL DBMSs <a class='headerlink' href='#section-nosql-dbmss' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="big-data-nosql-dbms-intro"><h1 id="big-data-nosql-dbms-intro-nosql-dbms">NoSQL DBMS</h1>
<p>Relational DBMS have lots of features like ACID properties, data integration and normalization schemas, standard model and query language and robustness.</p>
<p>However, a part from advantages, RDBMS have also weaknesses:</p>
<ul>
<li>Impedance mismatch (data are stored according to the relational model, but applications to modify them typically rely on the object-oriented model)</li>
<li>Painful scaling-out (not suited for a cluster architecture and distributed environments)</li>
<li>Consistency vs latency (today's applications require high reading/writing throughput with low latency)</li>
<li>Schema rigidity (schema evolution is often expensive)</li>
</ul>
<h2 id="big-data-nosql-dbms-intro-nosql-in-the-big-data-world">NoSQL in the Big Data world</h2>
<p>NoSQL systems are mainly used for operational workloads (<strong>OLTP</strong>).</p>
<p>Big data technologies are mainly used for analytical workloads (<strong>OLAP</strong>).</p>
<h2 id="big-data-nosql-dbms-intro-data-models">Data Models</h2>
<p>One of the key challenges is to understand which one fits best with the required application:</p>
<p><img alt="" src="big-data/nosql-dbms/data-model.jpg" /></p>
<p><strong>Graph Data Model</strong></p>
<p>Each database contains one or more graphs. 
Each graph contains <strong>vertices</strong> and <strong>arcs</strong>.</p>
<ul>
<li>Vertices: usually represent real-world entities</li>
<li>Arcs: represent direct relationships between the vertices</li>
</ul>
<p>The graph data model is intrinsically different form the others:</p>
<ul>
<li>It is focused on the relationships rather than on the entities per-se</li>
<li>It has limited <strong>scalability</strong> (it is often possible to shard a graph on several machines without cutting several arcs)</li>
<li>It is a data-driven modeling</li>
<li>It is based on the concept of <strong>encapsulation</strong></li>
</ul>
<p><strong>Document Data Model</strong></p>
<p>Each database contains one or more collections, each of which contains a list of documents (JSON).
Each document contains a set of fields and each field correspond to a key-value pair.</p>
<p>Depending on the <strong>query</strong> that I want to carry out, I can choose the most suitable model.</p>
<p><strong>Key-value Data Model</strong></p>
<p>Each database contains one or more collections and each collection contains a list of key-value pairs.</p>
<p><strong>Wide Column Data Model</strong></p>
<p>Each database contains one or more column families and each column family contains a list of row in the form of a key-value pair.</p>
<p>Each column is a key-value pair itself while rows specify only the columns for which a value exists.</p>
<p>The query language expressiveness is in between key-value and document data models.</p>
<h2 id="big-data-nosql-dbms-intro-sharding-data">Sharding Data</h2>
<p>One of the strengths of NoSQL systems is their <strong>scale-out capability</strong>.
Two aspects must be considered when deploying on a cluster:</p>
<ul>
<li>Sharding (subdividing data in shards that are stored in different machines)</li>
<li>Replication  (data is copied on several nodes)</li>
</ul>
<p><strong>Master-Slave Replication</strong></p>
<p>Master: manager of the data</p>
<ul>
<li>It handles each and every write operation</li>
<li>It can be chosen or drawn</li>
</ul>
<p>Slaves: enables read operations</p>
<ul>
<li>It is in sync with the master</li>
<li>It can become master if the latter fails</li>
</ul>
<p>Pros and cons of the master-slave replication:</p>
<p>PROS:</p>
<ul>
<li>Easy handles many read requests</li>
<li>Useful when the workload mainly consists of reads</li>
<li>Useful to avoid write conflicts</li>
</ul>
<p>CONS:</p>
<ul>
<li>The master is a bottleneck</li>
<li>Delay in write propagation can be a source of inconsistency</li>
<li>Not ideal when the workload mainly consists or writes</li>
</ul>
<p><strong>Peer-to-peer Replication</strong></p>
<p>Each node has the same importance and can handle write operations.
The loss of a node does not compromise reads nor writes.</p>
<p>Pros and cons of the peer-to-peer replication:</p>
<p>PROS:</p>
<ul>
<li>The failure of a node does not interrupt read or write requests</li>
<li>Write performances easily scale by adding new nodes</li>
</ul>
<p>CONS:</p>
<ul>
<li>Conflicts</li>
<li>Delay in write propagation can be a source of inconsistency</li>
<li>Two users may update the same value from different replicas</li>
</ul></section><section class="print-page" id="big-data-nosql-dbms-consistency"><h1 id="big-data-nosql-dbms-consistency-managing-consistency">Managing Consistency</h1>
<p>RDBMS come from decades of widespread usage with a strong focus on data consistency and years of research activities to optimize performances.</p>
<p>NoSQL systems are designed to succeed  where RDBMS fail:</p>
<ul>
<li>Strong focus on data sharding and high availability</li>
<li>Quite simple systems</li>
<li>Speed and manageability rather than consistency at all costs</li>
</ul>
<h2 id="big-data-nosql-dbms-consistency-cap-theorem">CAP Theorem</h2>
<p>According to the CAP theorem, only two of the following three properties can be guaranteed:</p>
<ul>
<li><strong>Consistency</strong> (every node returns the same, most recent, successful write)</li>
<li><strong>Availability</strong> </li>
<li><strong>Partition Tolerance</strong> (the system continues to function and upholds its consistency guarantees in spite of network partitions)</li>
</ul>
<p>In distributed systems, network partitioning is inevitably a possibility.</p>
<p>It sacrifices consistency over latency.</p>
<h2 id="big-data-nosql-dbms-consistency-pacelc-theorem">PACELC Theorem</h2>
<p>It is an evolution of the CAP theorem.</p>
<div class="highlight"><pre><span></span><code><span class="k">if</span><span class="w"> </span><span class="p">{</span><span class="n">Partition</span><span class="p">}</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="p">{</span><span class="n">Availability</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">Consistency</span><span class="o">?</span><span class="p">}</span><span class="w"></span>
<span class="n">Else</span><span class="w"> </span>
<span class="w">    </span><span class="p">{</span><span class="n">Latency</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">Consistency</span><span class="o">?</span><span class="p">}</span><span class="w"></span>
</code></pre></div>
<p><img alt="" src="big-data/nosql-dbms/consistency.jpg" /></p>
<p>Inconsistencies are tolerated in this area because latency is considered as being much more important.</p>
<p>For example, for Amazon, it is more important that users can navigate quickly rather than consistency in products data.</p>
<p>This because inconsistency does not impact user experience that much as latency would be.</p></section><h1 class='nav-section-title-end'>Ended: NoSQL DBMSs</h1>
                        <h1 class='nav-section-title' id='section-data-streaming'>
                            Data Streaming <a class='headerlink' href='#section-data-streaming' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="big-data-streaming-data-streaming"><h1 id="big-data-streaming-data-streaming-data-streaming">Data Streaming</h1>
<p>Big data is not just about <em>batch analysis</em>, but also about analyzing <strong>data streams</strong>.</p>
<p>Streaming data processing is a big deal in big data for many reasons:</p>
<ul>
<li>Latency (produce results in a more timely manner)</li>
<li>Workload balancing (distribute the workload in an efficient way)</li>
</ul>
<p>Despite this business-driven surge of interest in streaming, batch system are generally more mature than their streaming brethren.</p>
<p><strong>Use cases:</strong></p>
<ul>
<li>Operational monitoring</li>
<li>Web analytics</li>
<li>Online advertising</li>
<li>Social media</li>
<li>Intern of Things</li>
</ul>
<h2 id="big-data-streaming-data-streaming-real-time-data-streaming">Real time Data Streaming</h2>
<p><img alt="" src="big-data/streaming/real-time.jpg" /></p>
<p>For some systems, real-time is crucial. For example, a system that displays stock options has no tolerance for delay.</p>
<p>In some systems, delays would imply the total system failure (like with pacemaker).</p>
<h2 id="big-data-streaming-data-streaming-streaming-system">Streaming System</h2>
<p>The term streaming has been used for a variety of different things but in our context, a system for data streaming is a type of data processing engine that is designed with infinite datasets in mind.</p>
<p>Both batch engines and streaming engines can be used to process infinite datasets.</p>
<p><strong>Data Streaming Characteristics</strong>:</p>
<ul>
<li>Infinite dataset (no control over the order in which elements arrive)</li>
<li>Infinite computation (there must be a plan to avoid overflowing and enable auto-scalability)</li>
<li>Low-latency , approximate and speculative results<ul>
<li>Since we want to process data in real-time, data can usually be processes a single time</li>
<li>Only a fraction of the dataset can be kept in memory for analysis</li>
<li>Approximation may be required to accommodate the low-latency requirements</li>
</ul>
</li>
</ul>
<h2 id="big-data-streaming-data-streaming-data-stream-models">Data Stream Models</h2>
<ul>
<li><strong>Time series Model</strong> (stock prices)</li>
<li><strong>Cash Register Model</strong> (packages sent to IP addresses)</li>
<li><strong>Turnstile Model</strong> (people entering a subway)</li>
</ul></section><section class="print-page" id="big-data-streaming-architecture"><h1 id="big-data-streaming-architecture-architecture">Architecture</h1>
<p><img alt="" src="big-data/streaming/architecture.jpg" /></p>
<p>There is a lot of data movements among the different phases, which can be handles directly in the application that carry out the different activities or relying on a <strong>message queueing service</strong>.</p>
<p>This message queueing service works as a message bus connected to the single applications.</p>
<h2 id="big-data-streaming-architecture-message-queuing-service">Message Queuing Service</h2>
<p>The message queueing tier handles the transportation of data between different tiers.</p>
<ul>
<li>By decoupling the pipeline of operations, each node in the cluster will do one job only</li>
<li>Message queueing provides a solid framework for a safe communication between such nodes</li>
<li>Message queuing handles funneling of n data streams to m consumers</li>
</ul>
<p>In the message queueing system, we have he notion of a <strong>borker</strong>, which handles the communication, by means of queues.
The broker creates different queues, one for each topic, and keeps the data for a certain amount of time.</p>
<p><img alt="" src="big-data/streaming/broker.jpg" /></p>
<p>We can decide the level of <strong>delivery semantics</strong>, which refers to the level of precision in sending the message form producer to consumer.</p>
<ul>
<li><strong>Exactly once</strong>: a message is never lost and is read once and only once </li>
<li><strong>At most once</strong>: a message may get lost, but it will never be read twice</li>
<li><strong>At least once</strong>: a message will never be lost, but it may be read twice<ul>
<li>Less control but you avoid data getting lost (tradeoff)</li>
</ul>
</li>
</ul>
<h2 id="big-data-streaming-architecture-analysis">Analysis</h2>
<p>The analysis tier is the heart of the architecture and handles the analysis of the data.</p>
<p>In a <strong>continuous query model</strong>, you issue a query once and then it is continuously executed against the data.</p>
<ul>
<li>Memory constraints (data cannot be processed all together)</li>
<li>Time constraints (data that cannot be processed in time may have to be dropped) -&gt; <strong>load shedding</strong></li>
<li>Algorithms may ose efficacy over time -&gt; <strong>concept dirft</strong></li>
</ul>
<p><img alt="" src="big-data/streaming/query.jpg" /></p>
<p><strong>stateless query</strong> -&gt; each execution is independent from the other</p>
<p><strong>Distributed Execution</strong>: it refers to an architecture that is common to different frameworks.</p>
<p>We can consider three aspects:</p>
<ol>
<li>Message delivery semantics</li>
<li>State management (where to store intermediate state)</li>
<li>Fault tolerance <ul>
<li>Job replication (many nodes carry out the same job)</li>
<li>Rollback recovery (periodically store checkpoints and maintain a log of operations)</li>
</ul>
</li>
</ol>
<h2 id="big-data-streaming-architecture-windowing">Windowing</h2>
<p>Windowing techniques can be adopted to carry out analysis on a per-window basis (instead of a per-item basis).</p>
<p>In general, windows are defined by:</p>
<ul>
<li>Length (number of contained items)</li>
<li>Period (period after which the items in the window are processed)</li>
</ul>
<p>We can identify two types of windows:</p>
<ol>
<li><strong>Sliding windows</strong> (define length and period in terms of stream time)<ul>
<li>Fixed windows (length and period are the same)</li>
<li>Overlapping windows (length is greater than the period)</li>
<li>Sampling period (length is smaller than the period)</li>
</ul>
</li>
<li><strong>Data-driven windows</strong> (define the length in terms of the content that comes with the data)</li>
</ol>
<p><strong>Stream time and Event time</strong></p>
<p>Ideally, an event is processed exactly as it happens. In reality, there is a different between the time at which an event occurs (<strong>event time</strong>) and the time at which an event enters the streaming system (<strong>stream time</strong>).</p>
<p>The time difference is called <strong>skew</strong> and it may be due to:</p>
<ul>
<li>Hardware issues</li>
<li>Software issues</li>
</ul>
<p><strong>Windowing by stream time</strong></p>
<p>It has an extremely straightforward implementation, windows are always complete and it is possible to infer information about the source as it is observed.</p>
<p>On the other hand, event time cannot play an important role and, even if the data source sends events ordered by event item, it is not guaranteed that they arrive immediately or in the same order.</p>
<p><strong>Windowing by event time</strong></p>
<p>It is the gold standard of windowing.
However, it is impossible to precisely know when the window can be closed. Also, extended window lifetime implies more buffering of data and most processing systems lack native support.</p>
<p><strong>Triggers</strong> can be applied to declare intervals of intermediate processing of windows.
They can be based on:</p>
<ul>
<li>Watermark progress</li>
<li>Processing time progress</li>
<li>Element count</li>
<li>Record features</li>
</ul>
<p>Since late date (data that arrive after the watermark is defined) is realistic, a policy for <strong>allowed lateness</strong> must be defined.</p>
<h2 id="big-data-streaming-architecture-algorithms">Algorithms</h2>
<p>Consider <em>n</em> the space of events captured by the data stream.</p>
<p>Streaming algorithm have the following requirements:</p>
<ul>
<li>One-pass (once examined, items must be discarded)</li>
<li>Use small space for the internal state</li>
<li>Fast update of the internal state</li>
<li>Fast computation of answers</li>
<li>Provide approximated answers</li>
</ul>
<p>Analyzing a data stream is challenging due to space constraints, time constraints and algorithm infeasibility in one-pass.</p>
<p>One solution is to rely on <em>approximated algorithms</em>, where approximation can be achieved by relying on two main concepts:</p>
<ul>
<li>Sampling (sampling algorithms are known on time series and cash register models)</li>
<li>Random projections (relies on dimensionality reduction using projection along random vectors)</li>
</ul></section><section class="print-page" id="big-data-streaming-algorithms"><h1 id="big-data-streaming-algorithms-algorithms">Algorithms</h1>
<h2 id="big-data-streaming-algorithms-random-sampling-with-a-reservoir">Random Sampling with a Reservoir</h2>
<p>Random sampling is a common technique in data streaming.</p>
<p>Use case: <em>YouTube</em></p>
<p>The goal is to perform a real-time statistical analysis of the watched videos. On the surface it seems pretty easy, but, how do we know that data is random?</p>
<p><strong>Reservoir sampling</strong> is based on the notion of reservoir.
We cannot hold a predetermined number of stream values but, when a new value arrives, we can probabilistically determine whether to add it to out collection or to discard it.</p>
<p>The reservoir is always filled with the first <em>r</em> values in the stream, where r is the size of the reservoir.</p>
<p>For any element, the probability to go inside or outside the reservoir is the same.</p>
<p><img alt="" src="big-data/streaming/reservoir.jpg" /></p>
<h2 id="big-data-streaming-algorithms-hyperloglog">Hyperloglog</h2>
<p>Count distinct elements.</p>
<p>Use case: <em>Facebook</em></p>
<p>The goal is to count the users that are connected to the website.</p>
<p>There are two general categories for the <em>count-distinct</em> problem.
<strong>Hash functions</strong> can be used to translate input items to a uniform distribution in some bounded domains.</p>
<p><strong>Order statistics-based</strong> algorithms:</p>
<ul>
<li>Based on the evaluation of the order statistics</li>
<li>Since the distribution is uniform, we know that the lowest value is 1/number of elements</li>
<li>ONly the minimum value must be stored (no need to store hash tables)</li>
</ul>
<p><strong>Bit-pattern-based</strong> (HyperLogLog) algorithms:</p>
<p>The basis of the HyperLogLog algorithm is the following observation:</p>
<p>The cardinality of a multi-set of uniformly distributed random numbers can be estimated calculating the maximum number of leading zeros in the binary representation of each number in the set.</p>
<p>Given a binary string, the probability to observe:</p>
<ul>
<li>1 initial zero: 50%</li>
<li>2 initial zeros: 25%</li>
<li>3 initial zeros: 12.5%</li>
<li>n initial zeros: 1/n</li>
</ul>
<p>if I know the maximum number of initial zeros ever observed is n, than i can guess that it must have taken approximately 2^n attempts.</p>
<p><img alt="" src="big-data/streaming/hyperloglog.jpg" /></p>
<h2 id="big-data-streaming-algorithms-membership">Membership</h2>
<p>Use case: <em>YouTube</em></p>
<p>The goal is to identify whether a certain video has been viewed.</p>
<p>We can rely on an old data structure: <strong>Bloom filters</strong>.
A bloom filter can return false positives, but no false negatives.</p>
<p>A bloom filter is an array of m bits, with m &gt; n.
Incoming items are processed by a series of hash functions.</p>
<p>The optimal number of hash functions depends on the desired probability of false positives.
The optimal number of bits also depends on the number of expected elements.</p>
<h2 id="big-data-streaming-algorithms-frequency">Frequency</h2>
<p>Use case: <em>YouTube</em></p>
<p>We want to know how many times has a specific video been viewed today.</p>
<p>We want a space efficient structure and the answer relies on hashes and builds on the concept of Bloom filter.</p>
<p><strong>Count-Min Sketch</strong>: it is the most common algorithm for this problem</p>
<p><img alt="" src="big-data/streaming/count-min.jpg" /></p></section><h1 class='nav-section-title-end'>Ended: Data Streaming</h1>
                        <h1 class='nav-section-title' id='section-cloud-platforms-introduction'>
                            Cloud Platforms Introduction <a class='headerlink' href='#section-cloud-platforms-introduction' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="cloud-platforms-introduction-intro"><h1 id="cloud-platforms-introduction-intro-from-database-to-platform">From Database to Platform</h1>
<p>How did we get here?</p>
<ul>
<li><strong>Data-driven innovation</strong><ul>
<li>Data and analytics to foster new products, processes and markets</li>
<li>Drive discovery and execution of innovation, achieving new services with a business value</li>
</ul>
</li>
<li><strong>Analytics</strong><ul>
<li>Umbrella term that include different BI and application-related initiatives</li>
</ul>
</li>
<li><strong>Advanced analytics</strong><ul>
<li>Autonomous examination of data to discover deeper insights, make predictions, or generate recommendations</li>
</ul>
</li>
<li><strong>Augmented analytics</strong><ul>
<li>Use of technologies such as ML and AI to assist with data preparation, insight generation and insight explanation </li>
</ul>
</li>
</ul>
<p><img alt="" src="cloud-platforms/introduction/data.jpg" /></p>
<h2 id="cloud-platforms-introduction-intro-data-platform">Data Platform</h2>
<p>Getting <strong>value</strong> from data is not (only) a matter of storage.</p>
<p><strong>Database</strong></p>
<p>A database is a <strong>structured</strong> and persistent collection of information about some aspect of the real world organized and stored in a way that facilitates efficient retrieval and modification.
The structure of a database is determined by an abstract data model.</p>
<p>Operational databases are application-oriented.</p>
<p><strong>Data Warehouse</strong></p>
<p>A collection of data that supports decision-making processes.
It provides the following features:</p>
<ul>
<li>Subject-oriented</li>
<li>Integrated and consistent</li>
<li>Not volatile</li>
</ul>
<p>Depending on the application or the workload, we need to identify the most suitable data model to adopt.</p>
<p>Data warehouse allows Operational Data Storage, which integrate data from different applications.</p>
<p><strong>Data Lake</strong></p>
<p>In the data lake, we can save data of different format (raw data). By storing raw data, we do not need to design ETL techniques, as with DW, but we can simply store data as they are.</p>
<p>Data lakes are centralized repositories used for storage, processing, and analysis of raw data, in which data is kept in its original format and is processed to be queried only when needed.</p>
<p>They can store a varied amount of formats in big data ecosystems, from unstructured, semi-structured, to structured data sources.</p>
<p><img alt="" src="cloud-platforms/introduction/lake.jpg" /></p>
<p>Data warehouse is mainly relational, while data lakes are not.</p>
<p><img alt="" src="cloud-platforms/introduction/lake-vs-warehouse.jpg" /></p>
<p>Data lakes have increasingly taken the role of data hubs.
Drawing a sharp line between storage, computation and analysis is hard. Architectural borderlines are blurred and DL are often replaced by <strong>data platforms</strong> or <strong>ecosystem</strong>.</p>
<p><strong>Data Platform</strong></p>
<p>An integrated set of technologies that collectively meets an organization's end-to-end data needs such as acquisition, storage, preparation, delivery, and governance, as well as a security layer for users and applications.</p>
<p>We need to relieve users from complexity as they are only interested in the result.</p>
<p>Also, data transformation must be governed to prevent data platforms turning into a swamp.</p></section><section class="print-page" id="cloud-platforms-introduction-data-management"><h1 id="cloud-platforms-introduction-data-management-data-management">Data Management</h1>
<p>Requirements:</p>
<ul>
<li>Availability</li>
<li>Consistency / Integrity</li>
<li>Security</li>
<li>Scalability</li>
<li>Redundancy / Data Quality</li>
</ul>
<h2 id="cloud-platforms-introduction-data-management-data-provenance-lineage">Data Provenance (lineage)</h2>
<p>Data that describes the data by indicating the provenance.
As soon as data arrive in the database, we need to know its provenience.</p>
<p><em>Use Cases</em>: Research</p>
<p>Collecting data is expensive so the scientific field is moving towards more collaborative research and organizational boundaries are disappearing.</p>
<p>Sharing data and metadata across organizations is essential but you need to identify the sources in order to avoid bad quality data.</p>
<p>For example, astronomers are creating an <strong>international Virtual Observatory</strong>.</p>
<p>Compression of data can be either lossless of lossy. Compressing data in astronomy can result in data corruption.</p>
<p><img alt="" src="cloud-platforms/introduction/provenance.jpg" /></p>
<p>Decide the proper level of provenance is more art than science because storing too much data about provenance can be too burdensome.</p>
<p><strong>Granularity</strong></p>
<ul>
<li>Fine-grained (instance-level): tracking data items</li>
<li>Course-grained (schema-level): tracking dataset transformation</li>
</ul>
<p><strong>Queries</strong>:</p>
<ul>
<li>Where (which input did the output come from)</li>
<li>How (how were the inputs manipulated)</li>
<li>Why (why was data generated)</li>
</ul>
<p><img alt="" src="cloud-platforms/introduction/prov.jpg" /></p>
<h2 id="cloud-platforms-introduction-data-management-data-profiling">Data Profiling</h2>
<p>In a data lake we have many different resources that need to be profiled.</p>
<p>In a relational scenario, tables of a relational database are scanned to derive metadata, such as data types and value patterns, completeness and uniqueness of columns, key ad foreign keys, and occasionally, functional dependencies and association rules.</p>
<p>At some point, collecting and managing data features can become too complex.</p>
<p>Use cases:</p>
<ul>
<li>Query optimization</li>
<li>Data cleansing</li>
<li>Data integration and analytics</li>
</ul>
<p>However, the results of data profiling are computationally complex to discover.
Verification of complex constraints on column combinations in a database.</p>
<h2 id="cloud-platforms-introduction-data-management-data-versioning">Data Versioning</h2>
<p>Version control refers to a class of systems responsible for managing changes to computer programs, documents, or data collection.</p>
<p>Changes are identified by a number / letter code, termed the revision / version number.</p>
<p>However, data pipeline is not only about code but also about:</p>
<ul>
<li>Model Version control</li>
<li>Data version control</li>
<li>Model parameter tracking</li>
<li>Model performance comparison</li>
</ul>
<p>Data versioning support <strong>CRUD</strong> (Create, REad, Update, Delete) operations with versions.</p>
<p>Storing different versions of a program can result in storage issues. </p>
<p><img alt="" src="cloud-platforms/introduction/versioning.jpg" /></p>
<h2 id="cloud-platforms-introduction-data-management-compression-and-entity-resolution">Compression and Entity Resolution</h2>
<p><strong>Compression</strong> allows a concise representation of a dataset in a comprehensible and informative manner.</p>
<p><strong>Entity resolution</strong> finds records that refer to the same entity across different data sources.</p>
<p><em>Is using metadata enough?</em></p>
<p>No, metadata can become bigger than data themselves and data management is still a research issue in data platforms.</p>
<h2 id="cloud-platforms-introduction-data-management-data-lakehouse">Data Lakehouse</h2>
<p>Data lakehouse is a data management architecture that combines the flexibility, cost-efficiency, and scale of data lakes with the data management and ACID transactions of data warehouse, enabling business intelligence and machine learning on all data.</p>
<p>The idea of data lakehouse comes from <em>data bricks</em>: they analyze metadata internally but is subject to vendor lock in.</p>
<p><img alt="" src="cloud-platforms/introduction/lakehouse.jpg" /></p>
<h2 id="cloud-platforms-introduction-data-management-data-fabric">Data Fabric</h2>
<p>Frictionless access and sharing of data in a distributed data environment.</p>
<ul>
<li>It enables a single and consistent data management framework, which allows seamless data access and processing by design across otherwise siloed storage</li>
<li>Leverages human and machine capabilities to access data in place</li>
<li>Continuously identifies and connects data from disparate applications to discover unique relationships </li>
</ul>
<p>It is a unified <strong>architecture</strong> with an integrated set of technologies and services.</p>
<p>The idea is good but it cannot be applied to companies because, for example, data belonging to a country cannot be shared to other countries.</p>
<h2 id="cloud-platforms-introduction-data-management-data-mesh-hot-topic-super-new">Data Mesh - hot topic (super new)</h2>
<p>Distributed data architecture, under centralized governance and standardization for interoperability, enabled by a shared and harmonized self-serve data infrastructure.</p>
<p>In this case we have common guidelines that must be followed by everybody (domain-oriented decentralized data ownership).</p>
<h2 id="cloud-platforms-introduction-data-management-hadoop-based-data-platform">Hadoop-based data platform</h2>
<p><img alt="" src="cloud-platforms/introduction/hadoop.jpg" /></p>
<p>Many levels of complexity are hidden in these data platforms.</p>
<h2 id="cloud-platforms-introduction-data-management-data-platform-job-positions">Data Platform - Job Positions</h2>
<p><img alt="" src="cloud-platforms/introduction/jobs.jpg" /></p>
<h2 id="cloud-platforms-introduction-data-management-dataops">DataOps</h2>
<p>From DevOps to DataOps: a collaborative data management practice focused on improving the communication, integration and automation of data flows between data managers and data consumers across an organization.</p></section><h1 class='nav-section-title-end'>Ended: Cloud Platforms Introduction</h1>
                        <h1 class='nav-section-title' id='section-cloud-computing'>
                            Cloud Computing <a class='headerlink' href='#section-cloud-computing' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="cloud-platforms-cloud-computing-cloud-computing"><h1 id="cloud-platforms-cloud-computing-cloud-computing-cloud-computing">Cloud Computing</h1>
<p>Cloud solutions ae not only related to increased volume of data, but also to changes in the inflow of data.</p>
<p><strong>Cloud computing</strong> refers to a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction.</p>
<p>Cloud characteristics:</p>
<ul>
<li>On-demand self-service (consume services when you want)</li>
<li>Broad network access (consume services from anywhere)</li>
<li>Resource pooling (infrastructure, virtual platforms and applications)</li>
<li>Rapid elasticity (enable horizontal scalability)</li>
<li>Measures service (pay for the service you consume as you consume)</li>
</ul>
<p>Digital transformation involves the cloud to create/change business flows.</p>
<p>The main drivers of cloud computing are:</p>
<ul>
<li><strong>Scalability</strong> (not possible on premises)</li>
<li><strong>Elasticity</strong><ul>
<li>Automatically scale resources in response to run-time conditions</li>
<li>Adapt to changes in workload by turning on/off resources to match the necessary capacity</li>
<li>Core justification for the cloud adoption</li>
</ul>
</li>
<li><strong>Resource pooling</strong><ul>
<li>Cost-sharing (resources are dynamically reassigned according to demands)</li>
<li>Economy of scale</li>
<li>Based on virtualization, running multiple virtual instances on top of a physical computer system</li>
</ul>
</li>
<li><strong>Reliability</strong> (handle failures and highly available)</li>
<li><strong>Worldwide deployment</strong> </li>
<li>Measured <strong>quality of service</strong> (services leverage a quantitative qualitative metering capability making pay-as-you-go)</li>
<li><strong>Service integration</strong><ul>
<li>Abstract and automatically adapt the architecture to requirements</li>
<li>Integration and abstraction are drivers of change</li>
</ul>
</li>
</ul>
<p>Cloud computing is the outsourcing of a company's hardware and software architecture.
The risks involved include:</p>
<ul>
<li>Vendor Lock-in</li>
<li>Storage</li>
<li>No control over data (security)</li>
</ul>
<h2 id="cloud-platforms-cloud-computing-cloud-computing-types-of-service-providers">Types of Service Providers</h2>
<p>There different types of cloud:</p>
<ul>
<li><strong>Public</strong> (accessible to anyone like Microsoft, AWS, Google ...)</li>
<li><strong>Private</strong> (accessible by individuals within an institution)<ul>
<li>Cost-sharing disappears in private clouds</li>
<li>Security</li>
</ul>
</li>
<li><strong>Hybrid</strong> (a mix of the previous)</li>
</ul>
<p>Cloud services are hosted in separate geographic areas.
Locations are composed of regions (independent geographical area that groups data centers) and availability zones (data center).</p>
<p><strong>Main vendors</strong></p>
<p><img alt="" src="cloud-platforms/cloud-computing/gartner.jpg" /></p>
<h2 id="cloud-platforms-cloud-computing-cloud-computing-deployment-models">Deployment Models</h2>
<p>On a cloud architecture, you can rely on serverless or managed services.</p>
<p><strong>Serverless service</strong></p>
<ul>
<li>Standalone independent services built for a specific purpose and integrated by cloud service provider</li>
<li>No visibility into the machines</li>
<li>Pay for what your application uses, usually per request or per usage</li>
</ul>
<p><strong>(Fully) Managed service</strong></p>
<ul>
<li>Visibility and control of machines</li>
<li>Do not have to set up any machines, the management and backup are taken care for you</li>
<li>Pay for machine runtime no matter how long you run the machines and resources that your application uses</li>
</ul></section><h1 class='nav-section-title-end'>Ended: Cloud Computing</h1>
                        <h1 class='nav-section-title' id='section-aws-lab'>
                            AWS Lab <a class='headerlink' href='#section-aws-lab' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="cloud-platforms-aws-lab-california-housing"><h1 id="cloud-platforms-aws-lab-california-housing-california-house-pricing">California House Pricing</h1>
<p>Integrated lab: start from raw data and understand them through machine learning techniques</p>
<h2 id="cloud-platforms-aws-lab-california-housing-building-data-pipelines">Building Data Pipelines</h2>
<p>Following the CRISP methodology, the steps to undertake to build a data pipeline include:</p>
<ol>
<li>Frame the problem and look at the big picture</li>
<li>Get the data</li>
<li>Explore the data to gain insights</li>
<li>Prepare the data</li>
<li>Explore different models and find the best ones</li>
<li>Fine-tune your models</li>
<li>Present your solution</li>
<li>Launch, monitor, and maintain your system</li>
</ol>
<p><strong>Our Task:</strong></p>
<p>The goal is to predict a district's median housing price.</p>
<p>With continuous value, the type of analysis that we need to do is <strong>regression</strong>, while with qualitative data we need to do <strong>classification</strong>.</p>
<div class="highlight"><pre><span></span><code>Supervised: data have labels that describe them
Unsupervised: no labels
</code></pre></div>
<p>In this case, we will have to build a <strong>supervised</strong> model with <strong>regression</strong>.</p>
<p><img alt="" src="cloud-platforms/aws-lab/crisp.jpg" /></p>
<p>To measure the performance of our model, we should check the distance between our value and the real house price.</p>
<h2 id="cloud-platforms-aws-lab-california-housing-colab-notebook">Colab Notebook</h2>
<ol>
<li>Install dependencies (keep track of the libraries and their versions)<ul>
<li>Dependency management</li>
</ul>
</li>
<li>Read data<ul>
<li>Pandas</li>
<li>Sklearn</li>
</ul>
</li>
<li>Dataset exploration (understand the semantic)</li>
<li>Transform data (choose the right encoding system based on the data you have. We cannot use an ordinary encoding with nominal values like eye color)<ul>
<li>One hot encoding</li>
<li>Ordinary encoding </li>
</ul>
</li>
<li>How to handle missing values:<ul>
<li>Replace them with default values</li>
<li>Drop the row</li>
<li>Drop the column</li>
</ul>
</li>
<li>Check data types and change them to minimize memory occupation (from float64 to float32)</li>
</ol></section><h1 class='nav-section-title-end'>Ended: AWS Lab</h1></div>




              

  

            </article>
            
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
<script id="__config" type="application/json">{"base": "/", "features": [], "search": "assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    

<script src="https://unpkg.com/iframe-worker/polyfill"></script>
<script src="search/search_index.js"></script>


    
      <script src="assets/javascripts/bundle.9c69f0bc.min.js"></script>
      
        <script src="js/print-site.js"></script>
      
        <script src="js/arithmatex.config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>